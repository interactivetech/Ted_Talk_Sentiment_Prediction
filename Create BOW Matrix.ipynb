{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make BOW Matrix of TED Talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textacy\n",
    "import json\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\"data/transcripts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus=[]\n",
    "# docs = a['transcript'].tolist()\n",
    "# for doc in tqdm(docs):\n",
    "#     corpus.append(textacy.Doc(content=doc.decode('utf-8'),lang=u'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = textacy.Corpus(lang=u'en',texts=[i.decode('utf-8') for i in a['transcript'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2467x32966 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 1092422 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vectorizer = textacy.Vectorizer(tf_type='linear', apply_idf=False, idf_type='smooth',\n",
    "                                norm='l2', min_df=2, max_df=0.95)\n",
    "doc_term_matrix = vectorizer.fit_transform( (doc.to_terms_list(ngrams=1,\n",
    "                                                               named_entities=True,\n",
    "                                                               as_strings=True) for doc in c))\n",
    "print(repr(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Vectorizer in module textacy.vsm.vectorizers:\n",
      "\n",
      "class Vectorizer(__builtin__.object)\n",
      " |  Transform one or more tokenized documents into a sparse document-term matrix\n",
      " |  of shape (# docs, # unique terms), with flexibly weighted and normalized values.\n",
      " |  \n",
      " |  Stream a corpus with metadata from disk::\n",
      " |  \n",
      " |      >>> cw = textacy.datasets.CapitolWords()\n",
      " |      >>> text_stream, metadata_stream = textacy.io.split_records(\n",
      " |      ...     cw.records(limit=1000), 'text', itemwise=False)\n",
      " |      >>> corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
      " |      >>> corpus\n",
      " |      Corpus(1000 docs; 538172 tokens)\n",
      " |  \n",
      " |  Tokenize and vectorize the first 600 documents of this corpus::\n",
      " |  \n",
      " |      >>> tokenized_docs = (\n",
      " |      ...     doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
      " |      ...     for doc in corpus[:600])\n",
      " |      >>> vectorizer = Vectorizer(\n",
      " |      ...     apply_idf=True, norm='l2',\n",
      " |      ...     min_df=3, max_df=0.95)\n",
      " |      >>> doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
      " |      >>> doc_term_matrix\n",
      " |      <600x4346 sparse matrix of type '<class 'numpy.float64'>'\n",
      " |              with 69673 stored elements in Compressed Sparse Row format>\n",
      " |  \n",
      " |  Tokenize and vectorize the remaining 400 documents of the corpus, using only\n",
      " |  the groups, terms, and weights learned in the previous step::\n",
      " |  \n",
      " |      >>> tokenized_docs = (\n",
      " |      ...     doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
      " |      ...     for doc in corpus[600:])\n",
      " |      >>> doc_term_matrix = vectorizer.transform(tokenized_docs)\n",
      " |      >>> doc_term_matrix\n",
      " |      <400x4346 sparse matrix of type '<class 'numpy.float64'>'\n",
      " |              with 38756 stored elements in Compressed Sparse Row format>\n",
      " |  \n",
      " |  Inspect the terms associated with columns; they're sorted alphabetically::\n",
      " |  \n",
      " |      >>> vectorizer.terms_list[:5]\n",
      " |      ['', '$', '$ 1 million', '$ 1.2 billion', '$ 10 billion']\n",
      " |  \n",
      " |  (Btw: That empty string shouldn't be there. Somehow, spaCy is labeling it as\n",
      " |  a named entity...)\n",
      " |  \n",
      " |  If known in advance, limit the terms included in vectorized outputs\n",
      " |  to a particular set of values::\n",
      " |  \n",
      " |      >>> tokenized_docs = (\n",
      " |      ...     doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
      " |      ...     for doc in corpus[:600])\n",
      " |      >>> vectorizer = Vectorizer(\n",
      " |      ...     apply_idf=True, idf_type='smooth', norm='l2',\n",
      " |      ...     min_df=3, max_df=0.95,\n",
      " |      ...     vocabulary_terms=['president', 'bill', 'unanimous', 'distinguished', 'american'])\n",
      " |      >>> doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
      " |      >>> doc_term_matrix\n",
      " |      <600x5 sparse matrix of type '<class 'numpy.float64'>'\n",
      " |              with 844 stored elements in Compressed Sparse Row format>\n",
      " |      >>> vectorizer.terms_list\n",
      " |      ['american', 'bill', 'distinguished', 'president', 'unanimous']\n",
      " |  \n",
      " |  Specify different weighting schemes to determine values in the matrix,\n",
      " |  adding or customizing individual components, as desired::\n",
      " |  \n",
      " |      >>> money_idx = vectorizer.vocabulary_terms['$']\n",
      " |      >>> doc_term_matrix = Vectorizer(\n",
      " |      ...     tf_type='linear', norm=None, min_df=3, max_df=0.95\n",
      " |      ...     ).fit_transform(tokenized_docs)\n",
      " |      >>> print(doc_term_matrix[0:7, money_idx].toarray())\n",
      " |      [[0]\n",
      " |       [0]\n",
      " |       [1]\n",
      " |       [4]\n",
      " |       [0]\n",
      " |       [0]\n",
      " |       [2]]\n",
      " |      >>> doc_term_matrix = Vectorizer(\n",
      " |      ...     tf_type='sqrt', apply_dl=True, dl_type='sqrt', norm=None, min_df=3, max_df=0.95\n",
      " |      ...     ).fit_transform(tokenized_docs)\n",
      " |      >>> print(doc_term_matrix[0:7, money_idx].toarray())\n",
      " |      [[0.        ]\n",
      " |       [0.        ]\n",
      " |       [0.10101525]\n",
      " |       [0.26037782]\n",
      " |       [0.        ]\n",
      " |       [0.        ]\n",
      " |       [0.11396058]]\n",
      " |      >>> doc_term_matrix = Vectorizer(\n",
      " |      ...     tf_type='bm25', apply_idf=True, idf_type='smooth', norm=None, min_df=3, max_df=0.95\n",
      " |      ...     ).fit_transform(tokenized_docs)\n",
      " |      >>> print(doc_term_matrix[0:7, money_idx].toarray())\n",
      " |      [[0.        ]\n",
      " |       [0.        ]\n",
      " |       [3.28353965]\n",
      " |       [5.82763722]\n",
      " |       [0.        ]\n",
      " |       [0.        ]\n",
      " |       [4.83933924]]\n",
      " |  \n",
      " |  If you're not sure what's going on mathematically, :attr:`Vectorizer.weighting`\n",
      " |  gives the formula being used to calculate weights, based on the parameters\n",
      " |  set when initializing the vectorizer::\n",
      " |  \n",
      " |      >>> vectorizer.weighting\n",
      " |      '(tf * (k + 1)) / (k + tf) * log((n_docs + 1) / (df + 1)) + 1'\n",
      " |  \n",
      " |  In general, weights may consist of a local component (term frequency),\n",
      " |  a global component (inverse document frequency), and a normalization\n",
      " |  component (document length). Individual components may be modified:\n",
      " |  they may have different scaling (e.g. tf vs. sqrt(tf)) or different behaviors\n",
      " |  (e.g. \"standard\" idf vs bm25's version). There are *many* possible weightings,\n",
      " |  and some may be better for particular use cases than others. When in doubt,\n",
      " |  though, just go with something standard.\n",
      " |  \n",
      " |  - \"tf\": Weights are simply the absolute per-document term frequencies (tfs),\n",
      " |    i.e. value (i, j) in an output doc-term matrix corresponds to the number\n",
      " |    of occurrences of term j in doc i. Terms appearing many times in a given\n",
      " |    doc receive higher weights than less common terms.\n",
      " |    Params: ``tf_type='linear', apply_idf=False, apply_dl=False``\n",
      " |  - \"tfidf\": Doc-specific, *local* tfs are multiplied by their corpus-wide,\n",
      " |    *global* inverse document frequencies (idfs). Terms appearing in many docs\n",
      " |    have higher document frequencies (dfs), correspondingly smaller idfs, and\n",
      " |    in turn, lower weights.\n",
      " |    Params: ``tf_type='linear', apply_idf=True, idf_type='smooth', apply_dl=False``\n",
      " |  - \"bm25\": This scheme includes a local tf component that increases asymptotically,\n",
      " |    so higher tfs have diminishing effects on the overall weight; a global idf\n",
      " |    component that can go *negative* for terms that appear in a sufficiently\n",
      " |    high proportion of docs; as well as a row-wise normalization that accounts for\n",
      " |    document length, such that terms in shorter docs hit the tf asymptote sooner\n",
      " |    than those in longer docs.\n",
      " |    Params: ``tf_type='bm25', apply_idf=True, idf_type='bm25', apply_dl=True``\n",
      " |  - \"binary\": This weighting scheme simply replaces all non-zero tfs with 1,\n",
      " |    indicating the presence or absence of a term in a particular doc. That's it.\n",
      " |    Params: ``tf_type='binary', apply_idf=False, apply_dl=False``\n",
      " |  \n",
      " |  Slightly altered versions of these \"standard\" weighting schemes are common,\n",
      " |  and may have better behavior in general use cases:\n",
      " |  \n",
      " |  - \"lucene-style tfidf\": Adds a doc-length normalization to the usual local\n",
      " |    and global components.\n",
      " |    Params: ``tf_type='linear', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='sqrt'``\n",
      " |  - \"lucene-style bm25\": Uses a smoothed idf instead of the classic bm25 variant\n",
      " |    to prevent weights on terms from going negative.\n",
      " |    Params: ``tf_type='bm25', apply_idf=True, idf_type='smooth', apply_dl=True, dl_type='linear'``\n",
      " |  \n",
      " |  Args:\n",
      " |      tf_type ({'linear', 'sqrt', 'log', 'binary'}): Type of term frequency (tf)\n",
      " |          to use for weights' local component:\n",
      " |  \n",
      " |          - 'linear': tf (tfs are already linear, so left as-is)\n",
      " |          - 'sqrt': tf => sqrt(tf)\n",
      " |          - 'log': tf => log(tf) + 1\n",
      " |          - 'binary': tf => 1\n",
      " |  \n",
      " |      apply_idf (bool): If True, apply global idfs to local term weights, i.e.\n",
      " |          divide per-doc term frequencies by the (log of the) total number\n",
      " |          of documents in which they appear; otherwise, don't.\n",
      " |      idf_type ({'standard', 'smooth', 'bm25'}): Type of inverse document\n",
      " |          frequency (idf) to use for weights' global component:\n",
      " |  \n",
      " |          - 'standard': idf = log(n_docs / df) + 1.0\n",
      " |          - 'smooth': idf = log(n_docs + 1 / df + 1) + 1.0, i.e. 1 is added\n",
      " |            to all document frequencies, as if a single document containing\n",
      " |            every unique term was added to the corpus. This prevents zero divisions!\n",
      " |          - 'bm25': idf = log((n_docs - df + 0.5) / (df + 0.5)), which is\n",
      " |            a form commonly used in information retrieval that allows for\n",
      " |            very common terms to receive negative weights.\n",
      " |  \n",
      " |      apply_dl (bool): If True, normalize local(+global) weights by doc length,\n",
      " |          i.e. divide by the total number of in-vocabulary terms appearing\n",
      " |          in a given doc; otherwise, don't.\n",
      " |      dl_type ({'linear', 'sqrt', 'log'}): Type of document-length scaling\n",
      " |          to use for weights' normalization component:\n",
      " |  \n",
      " |          - 'linear': dl (dls are already linear, so left as-is)\n",
      " |          - 'sqrt': dl => sqrt(dl)\n",
      " |          - 'log': dl => log(dl)\n",
      " |  \n",
      " |      norm ({'l1', 'l2'} or None): If 'l1' or 'l2', normalize weights by the\n",
      " |          L1 or L2 norms, respectively, of row-wise vectors; otherwise, don't.\n",
      " |      vocabulary_terms (Dict[str, int] or Iterable[str]): Mapping of unique term\n",
      " |          string to unique term id, or an iterable of term strings that gets\n",
      " |          converted into a suitable mapping. Note that, if specified, vectorized\n",
      " |          outputs will include *only* these terms as columns.\n",
      " |      min_df (float or int): If float, value is the fractional proportion of\n",
      " |          the total number of documents, which must be in [0.0, 1.0]. If int,\n",
      " |          value is the absolute number. Filter terms whose document frequency\n",
      " |          is less than ``min_df``.\n",
      " |      max_df (float or int): If float, value is the fractional proportion of\n",
      " |          the total number of documents, which must be in [0.0, 1.0]. If int,\n",
      " |          value is the absolute number. Filter terms whose document frequency\n",
      " |          is greater than ``max_df``.\n",
      " |      max_n_terms (int): Only include terms whose document frequency is within\n",
      " |          the top ``max_n_terms``.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      vocabulary_terms (Dict[str, int]): Mapping of unique term string to unique\n",
      " |          term id, either provided on instantiation or generated by calling\n",
      " |          :meth:`Vectorizer.fit()` on a collection of tokenized documents.\n",
      " |      id_to_term (Dict[int, str]): Mapping of unique term id to unique term\n",
      " |          string, i.e. the inverse of :attr:`Vectorizer.vocabulary_terms`.\n",
      " |          This mapping is only generated as needed.\n",
      " |      terms_list (List[str]): List of term strings in column order of\n",
      " |          vectorized outputs.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, tf_type=u'linear', apply_idf=False, idf_type=u'smooth', apply_dl=False, dl_type=u'sqrt', norm=None, min_df=1, max_df=1.0, max_n_terms=None, vocabulary_terms=None)\n",
      " |  \n",
      " |  fit(self, tokenized_docs)\n",
      " |      Count terms in ``tokenized_docs`` and, if not already provided, build up\n",
      " |      a vocabulary based those terms. Fit and store global weights (IDFs)\n",
      " |      and, if needed for term weighting, the average document length.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized\n",
      " |              documents, where each is a sequence of (str) terms. For example::\n",
      " |      \n",
      " |                  >>> ([tok.lemma_ for tok in spacy_doc]\n",
      " |                  ...  for spacy_doc in spacy_docs)\n",
      " |                  >>> ((ne.text for ne in extract.named_entities(doc))\n",
      " |                  ...  for doc in corpus)\n",
      " |                  >>> (doc.to_terms_list(as_strings=True)\n",
      " |                  ...  for doc in docs)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Vectorizer`: The instance that has just been fit.\n",
      " |  \n",
      " |  fit_transform(self, tokenized_docs)\n",
      " |      Count terms in ``tokenized_docs`` and, if not already provided, build up\n",
      " |      a vocabulary based those terms. Fit and store global weights (IDFs)\n",
      " |      and, if needed for term weighting, the average document length.\n",
      " |      Transform ``tokenized_docs`` into a document-term matrix with values\n",
      " |      weighted according to the parameters in :class:`Vectorizer` initialization.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized\n",
      " |              documents, where each is a sequence of (str) terms. For example::\n",
      " |      \n",
      " |                  >>> ([tok.lemma_ for tok in spacy_doc]\n",
      " |                  ...  for spacy_doc in spacy_docs)\n",
      " |                  >>> ((ne.text for ne in extract.named_entities(doc))\n",
      " |                  ...  for doc in corpus)\n",
      " |                  >>> (doc.to_terms_list(as_strings=True)\n",
      " |                  ...  for doc in docs)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`scipy.sparse.csr_matrix`: The transformed document-term matrix.\n",
      " |          Rows correspond to documents and columns correspond to terms.\n",
      " |  \n",
      " |  transform(self, tokenized_docs)\n",
      " |      Transform ``tokenized_docs`` into a document-term matrix with values\n",
      " |      weighted according to the parameters in :class:`Vectorizer` initialization\n",
      " |      and the global weights computed by calling :meth:`Vectorizer.fit()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokenized_docs (Iterable[Iterable[str]]): A sequence of tokenized\n",
      " |              documents, where each is a sequence of (str) terms. For example::\n",
      " |      \n",
      " |                  >>> ([tok.lemma_ for tok in spacy_doc]\n",
      " |                  ...  for spacy_doc in spacy_docs)\n",
      " |                  >>> ((ne.text for ne in extract.named_entities(doc))\n",
      " |                  ...  for doc in corpus)\n",
      " |                  >>> (doc.to_terms_list(as_strings=True)\n",
      " |                  ...  for doc in docs)\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`scipy.sparse.csr_matrix`: The transformed document-term matrix.\n",
      " |          Rows correspond to documents and columns correspond to terms.\n",
      " |      \n",
      " |      Note:\n",
      " |          For best results, the tokenization used to produce ``tokenized_docs``\n",
      " |          should be the same as was applied to the docs used in fitting this\n",
      " |          vectorizer or in generating a fixed input vocabulary.\n",
      " |      \n",
      " |          Consider an extreme case where the docs used in fitting consist of\n",
      " |          lowercased (non-numeric) terms, while the docs to be transformed are\n",
      " |          all uppercased: The output doc-term-matrix will be empty.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  id_to_term\n",
      " |      dict: Mapping of unique term id (int) to unique term string (str), i.e.\n",
      " |          the inverse of :attr:`Vectorizer.vocabulary`. This attribute is only\n",
      " |          generated if needed, and it is automatically kept in sync with the\n",
      " |          corresponding vocabulary.\n",
      " |  \n",
      " |  terms_list\n",
      " |      List of term strings in column order of vectorized outputs. For example,\n",
      " |      ``terms_list[0]`` gives the term assigned to the first column in an\n",
      " |      output doc-term-matrix, ``doc_term_matrix[:, 0]``.\n",
      " |  \n",
      " |  weighting\n",
      " |      str: A mathematical representation of the overall weighting scheme\n",
      " |      used to determine values in the vectorized matrix, depending on the\n",
      " |      params used to initialize the :class:`Vectorizer`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textacy.Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 597 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   587  588  589  590  \\\n",
       "0    0    0    0    0    0    1    2    0    0    2 ...     1    7    2    6   \n",
       "1    0    0    0    0    0    0    0    0    0    1 ...     0    1    1    2   \n",
       "2    3    1    0    2    2    1    0    0    0    0 ...     1    1    9    2   \n",
       "3    3    0    0    0    0    1    0    0    2    0 ...     1    3    1    1   \n",
       "4    1    4    3    0    0    6    1    1    0    0 ...     0    0    1    0   \n",
       "5    1    5    2    2    1    0    3    2    0    2 ...     0    3    4    1   \n",
       "6    0    1    0    0    0    0    0    0    0    0 ...     0    5    1    1   \n",
       "7    0    0    0    0    0    1    3    1    1    0 ...     0    1    0    0   \n",
       "8    0    0    1    0    0    0    0    0    0    1 ...     2    4    1    3   \n",
       "9    0    2    0    1    1    0    2    1    1    0 ...     3    2    4    0   \n",
       "\n",
       "   591  592  593  594  595  596  \n",
       "0    1    8    0    2    0    0  \n",
       "1    0    0    1    1    0    1  \n",
       "2    1    8    2    0    1    0  \n",
       "3    1    2    0    0    5    1  \n",
       "4    0   11    0    0    0    0  \n",
       "5    0   13    2    1    3    0  \n",
       "6    4    6    8    0    0    1  \n",
       "7    0    5    0    0    1    0  \n",
       "8    0   16    3    1    0    0  \n",
       "9    0   14    0    1    0    0  \n",
       "\n",
       "[10 rows x 597 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True) for doc in corpus)\n",
    "vectorizer = textacy.Vectorizer(apply_idf=False,min_df=3, max_df=0.95)\n",
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "BOW = pd.DataFrame(doc_term_matrix.toarray())\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW.to_csv(\"data/BOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019233</td>\n",
       "      <td>0.038466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019233</td>\n",
       "      <td>0.091811</td>\n",
       "      <td>0.026232</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.104927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022611</td>\n",
       "      <td>0.022611</td>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033156</td>\n",
       "      <td>0.033156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065163</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048863</td>\n",
       "      <td>0.048863</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019507</td>\n",
       "      <td>0.013303</td>\n",
       "      <td>0.119725</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.106422</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.057436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.035175</td>\n",
       "      <td>0.011725</td>\n",
       "      <td>0.014114</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095726</td>\n",
       "      <td>0.021534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.058183</td>\n",
       "      <td>0.054653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087274</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>0.016197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.017214</td>\n",
       "      <td>0.077294</td>\n",
       "      <td>0.038723</td>\n",
       "      <td>0.038723</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046376</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137048</td>\n",
       "      <td>0.030918</td>\n",
       "      <td>0.015459</td>\n",
       "      <td>0.051641</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050940</td>\n",
       "      <td>0.010188</td>\n",
       "      <td>0.012264</td>\n",
       "      <td>0.066542</td>\n",
       "      <td>0.061128</td>\n",
       "      <td>0.119516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.047932</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031416</td>\n",
       "      <td>0.042849</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.038684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171396</td>\n",
       "      <td>0.047125</td>\n",
       "      <td>0.015708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048968</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>0.044525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 597 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.019233  0.038466   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.065163  0.019507  0.000000  0.048863  0.048863  0.019507  0.000000   \n",
       "3  0.057436  0.000000  0.000000  0.000000  0.000000  0.017193  0.000000   \n",
       "4  0.016197  0.058183  0.054653  0.000000  0.000000  0.087274  0.014546   \n",
       "5  0.017214  0.077294  0.038723  0.038723  0.019361  0.000000  0.046376   \n",
       "6  0.000000  0.014940  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.015977  0.047932   \n",
       "8  0.000000  0.000000  0.019674  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.032645  0.000000  0.020443  0.020443  0.000000  0.032645   \n",
       "\n",
       "        7         8         9      ...          587       588       589  \\\n",
       "0  0.000000  0.000000  0.042832    ...     0.019233  0.091811  0.026232   \n",
       "1  0.000000  0.000000  0.036920    ...     0.000000  0.022611  0.022611   \n",
       "2  0.000000  0.000000  0.000000    ...     0.019507  0.013303  0.119725   \n",
       "3  0.000000  0.043068  0.000000    ...     0.017193  0.035175  0.011725   \n",
       "4  0.016197  0.000000  0.000000    ...     0.000000  0.000000  0.009919   \n",
       "5  0.034427  0.000000  0.034427    ...     0.000000  0.031627  0.042169   \n",
       "6  0.000000  0.000000  0.000000    ...     0.000000  0.050940  0.010188   \n",
       "7  0.017791  0.020011  0.000000    ...     0.000000  0.010896  0.000000   \n",
       "8  0.000000  0.000000  0.017491    ...     0.031416  0.042849  0.010712   \n",
       "9  0.018175  0.020443  0.000000    ...     0.048968  0.022262  0.044525   \n",
       "\n",
       "        590       591       592       593       594       595       596  \n",
       "0  0.094728  0.021416  0.104927  0.000000  0.038466  0.000000  0.000000  \n",
       "1  0.054435  0.000000  0.000000  0.033156  0.033156  0.000000  0.041527  \n",
       "2  0.032026  0.021721  0.106422  0.039014  0.000000  0.021721  0.000000  \n",
       "3  0.014114  0.019145  0.023450  0.000000  0.000000  0.095726  0.021534  \n",
       "4  0.000000  0.000000  0.109114  0.000000  0.000000  0.000000  0.000000  \n",
       "5  0.012690  0.000000  0.137048  0.030918  0.015459  0.051641  0.000000  \n",
       "6  0.012264  0.066542  0.061128  0.119516  0.000000  0.000000  0.018711  \n",
       "7  0.000000  0.000000  0.054479  0.000000  0.000000  0.017791  0.000000  \n",
       "8  0.038684  0.000000  0.171396  0.047125  0.015708  0.000000  0.000000  \n",
       "9  0.000000  0.000000  0.155837  0.000000  0.016323  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 597 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True) for doc in corpus)\n",
    "vectorizer = textacy.Vectorizer(apply_idf=True, norm='l2',min_df=3, max_df=0.95)\n",
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "BOW_tf = pd.DataFrame(doc_term_matrix.toarray())\n",
    "BOW_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tf.to_csv(\"data/tf-idf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
