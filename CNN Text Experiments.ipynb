{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import textacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Splitting\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# ast.literal_eval\n",
    "# df = get_undersampled(pd.read_csv(\"data/train_raw.csv\"))\n",
    "# df2 = get_undersampled(pd.read_csv(\"data/test_raw.csv\"))\n",
    "\n",
    "df = pd.read_csv(\"data/train_raw.csv\")\n",
    "df2 = pd.read_csv(\"data/test_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags =df[\"transcript\"]\n",
    "test_tags = df2[\"transcript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = df\n",
    "# # labels.head()\n",
    "# test_labels = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tags.tolist()[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "train_corp = textacy.Corpus(lang='en')\n",
    "for l in tqdm(train_tags.tolist()):\n",
    "    train_corp.add_text(l)\n",
    "test_corp = textacy.Corpus(lang='en')\n",
    "for l in tqdm(test_tags.tolist()):\n",
    "    test_corp.add_text(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entire corpus\n",
    "TR_LEN = len(train_corp)\n",
    "TE_LEN =len(test_corp)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire_corpus = textacy.Corpus(lang='en')\n",
    "# for l in train_tags.tolist()+test_tags.tolist():\n",
    "#     entire_corpus.add_text(' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(entire_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(test_corp[0].tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data - Without combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_corpus(train_c,test_c):\n",
    "    vectorizer = textacy.Vectorizer(\n",
    "        tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "        min_df=2, max_df=0.95)\n",
    "    train = vectorizer.fit_transform(\n",
    "        (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "         for doc in train_c))\n",
    "    test = vectorizer.transform(\n",
    "        (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "         for doc in test_c))\n",
    "    return train,test\n",
    "X_train,X_test = tf_idf_corpus(train_corp,test_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize data with combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_idf_corpus(corpus):\n",
    "#     vectorizer = textacy.Vectorizer(\n",
    "#         tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "#         min_df=2, max_df=0.95)\n",
    "#     doc_term_matrix = vectorizer.fit_transform(\n",
    "#         (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "#          for doc in corpus))\n",
    "#     return doc_term_matrix\n",
    "# X = tf_idf_corpus(entire_corpus)\n",
    "# X_train=X[:TR_LEN]\n",
    "# X_test = X[TR_LEN:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_logreg_results(X_train, X_test, y_train, y_test):\n",
    "#     print(\"Training Logistic Regression\")\n",
    "#     clf = LogisticRegression()\n",
    "#     skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "#     scores = []\n",
    "#     f = 0\n",
    "#     for train_index, test_index in skf.split(X_train, y_train):\n",
    "#         X_tr, X_t = X_train[train_index], X_train[test_index]\n",
    "#         y_tr, y_t = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "#         clf.fit(X_tr, y_tr)\n",
    "#         scores.append(clf.score(X_t, y_t))\n",
    "#         print(\"Fold {}: {}\".format(f+1, scores[-1]))\n",
    "#         f+=1\n",
    "#     print(\"Logistic cross-validation accuracy: {}\".format(np.mean(scores)))\n",
    "    \n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print(\"Logistic accuracy on the test set: {}\".format(accuracy_score(y_test, clf.predict(X_test))))\n",
    "    \n",
    "# def get_svm_results(X_train, X_test, y_train, y_test):\n",
    "#     print(\"Training SVM\")\n",
    "\n",
    "#     clf = SVC(kernel='linear')\n",
    "#     skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "#     scores = []\n",
    "#     f = 0\n",
    "#     for train_index, test_index in skf.split(X_train, y_train):\n",
    "#         X_tr, X_t = X_train[train_index], X_train[test_index]\n",
    "#         y_tr, y_t = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "#         clf.fit(X_tr, y_tr)\n",
    "#         scores.append(clf.score(X_t, y_t))\n",
    "#         print(\"Fold {}: {}\".format(f+1, scores[-1]))\n",
    "#         f+=1\n",
    "#     print(\"SVM cross-validation accuracy: {}\".format(np.mean(scores)))\n",
    "    \n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print(\"SVM accuracy on the test set: {}\".format(accuracy_score(y_test, clf.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"label\"].as_matrix()\n",
    "y_test = df2[\"label\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_logreg_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_svm_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_logreg_results(X, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_svm_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def get_undersampled(df, threshold=200, window=10):\n",
    "#         indices = df['label'].value_counts()[df['label'].value_counts() > threshold].index.tolist()\n",
    "\n",
    "#         for ind in indices:\n",
    "#             over_df = df[df['label'] == ind].reset_index(drop=True)\n",
    "#             df = df.drop(df[df['label'] == ind].index)\n",
    "#             to_drop = np.random.randint(threshold-window, \n",
    "#                                         threshold+window)\n",
    "\n",
    "#             trans_ids = np.random.choice(range(len(over_df)),\n",
    "#                                          to_drop)\n",
    "\n",
    "#             dfs_to_add = over_df.iloc[trans_ids]\n",
    "#             df = pd.concat([df, dfs_to_add]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#         # Shuffle result\n",
    "#         df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#         return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Text Classification, no undersamplng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp train_5500.label train_5500.label.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# def flatten2(l):\n",
    "#     res = []\n",
    "#     for elem in l:\n",
    "#         for item in elem.split(' '):\n",
    "# #             a.append(item)\n",
    "#             res.append(item)\n",
    "#     return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = open('train_5500.label.txt', 'r', encoding='latin-1').readlines()\n",
    "\n",
    "# data = [[d.split(':')[1][:-1], d.split(':')[0]] for d in data]\n",
    "\n",
    "# X_2, y_2 = list(zip(*data))\n",
    "# vocab = list(set(flatten2(X_2)))#converts sentences into sequence of characters, and finds unique set of characters\n",
    "# X_2 = [i.split(' ') for i in X_2 ]\n",
    "# ### Num masking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocabulary of entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "res =[a.update(train_corp[i].to_bag_of_terms(ngrams=1, as_strings=True))  for i in tqdm(range(len(train_corp)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={'<PAD>': 0, '<UNK>': 1}\n",
    "for key in tqdm(a.keys()):\n",
    "    if key not in word2index:\n",
    "        word2index[key] = len(word2index)\n",
    "index2word = {v:k for k, v in word2index.items()}#loop through sets, and set the value as index, and key as value\n",
    "\n",
    "# word2index2={'<PAD>': 0, '<UNK>': 1}\n",
    "# for key in vocab:\n",
    "#     if key not in word2index2:\n",
    "#         word2index2[key] = len(word2index2)\n",
    "# index2word2 = {v:k for k, v in word2index2.items()}#loop through sets, and set the value as index, and key as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Label Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2index = {}\n",
    "\n",
    "for cl in set(train_labels):\n",
    "    if target2index.get(cl) is None:\n",
    "        target2index[cl] = len(target2index)# assign an index to unique labels\n",
    "\n",
    "index2target = {v:k for k, v in target2index.items()}\n",
    "\n",
    "\n",
    "# target2index2 = {}\n",
    "\n",
    "# for cl in set(y_2):\n",
    "#     if target2index2.get(cl) is None:\n",
    "#         target2index2[cl] = len(target2index2)# assign an index to unique labels\n",
    "\n",
    "# index2target2 = {v:k for k, v in target2index2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X+=[list(train_corp[i].tokens) for i in tqdm(range(len(train_corp)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "\n",
    "def pad_to_batch(batch):\n",
    "    x,y = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_x = max(9,max_x)\n",
    "\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "#             print(x[i].size(1))\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1)\n",
    "\n",
    "def pad_sample(batch):\n",
    "    max_x = max([s.size(1) for s in batch])\n",
    "#     max_x = max(5,max_x)\n",
    "\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "#             print(x[i].size(1))\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1)\n",
    "\n",
    "def prepare_sequence(seq, to_index):\n",
    "    '''\n",
    "    Converts list of tokens in to list of indicies\n",
    "    '''\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "def prepare_sequence_textacy(seq, to_index):\n",
    "    '''\n",
    "    Converts list of tokens in to list of indicies\n",
    "    '''\n",
    "    idxs = list(map(lambda w: to_index[str(w)] if to_index.get((str(w))) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "def prepare_sequence2(seq, to_index):\n",
    "    '''\n",
    "    Converts list of tokens in to list of indicies\n",
    "    '''\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(X_2,y_2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check = list(zip(X_2,y_2))\n",
    "# print([w for w in check[0][0]])\n",
    "# print(word2index2.get('n'))\n",
    "# list(map(lambda w: word2index2[w] if word2index2.get(w) is not None else word2index2[\"<UNK>\"], check[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_sequence(X_2[0][0], word2index2).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2index2['m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2index[str(X[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X\n",
    "prepare_sequence_textacy(X[4], word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, y_p = [], []\n",
    "for pair in tqdm(zip(X,train_labels)):\n",
    "    X_p.append(prepare_sequence_textacy(pair[0], word2index).view(1, -1))\n",
    "#     print(X_p)\n",
    "    y_p.append(Variable(LongTensor([target2index[pair[1]]])).view(1, -1))\n",
    "    \n",
    "data_p = list(zip(X_p, y_p))\n",
    "random.shuffle(data_p)\n",
    "\n",
    "train_data = data_p[: int(len(data_p) * 0.8)]\n",
    "test_data = data_p[int(len(data_p) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_p, y_p = [], []\n",
    "# for pair in zip(X_2,y_2):\n",
    "#     X_p.append(prepare_sequence(pair[0], word2index2).view(1, -1))\n",
    "#     y_p.append(Variable(LongTensor([target2index2[pair[1]]])).view(1, -1))\n",
    "\n",
    "\n",
    "# data_p2 = list(zip(X_p, y_p))\n",
    "# # print([i[0].shape[1] for i in data_p2 if i[0].shape[1] < 4])\n",
    "# # print(data_p2)\n",
    "# # random.shuffle(data_p2)\n",
    "\n",
    "# train_data2 = data_p2[: int(len(data_p) * 0.9)]#BE CAREFUL WITH REUSING VARIABLES\n",
    "# test_data2 = data_p2[int(len(data_p) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data2[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = []\n",
    "for key in tqdm(index2word.keys()):\n",
    "    '''\n",
    "    Here for all the words in vocab, return index, \n",
    "    then find word embedding associated with that word\n",
    "    \n",
    "    NOTE: THIS ONLY WORKS BECAUSE WHEN MAKING WORD2INDEX, \n",
    "    VALUES GUARENTEED TO BE IN ASCENDING ORDER\n",
    "    '''\n",
    "#     print(key)\n",
    "    try:\n",
    "#         print(index2word[key],model[index2word[key]].shape)\n",
    "        pretrained.append(model2[index2word[key]])\n",
    "    except:\n",
    "#         print(\"Random\")\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For dummy dataset\n",
    "# pretrained = []\n",
    "# for key in index2word2.keys():\n",
    "#     '''\n",
    "#     Here for all the words in vocab, return index, \n",
    "#     then find word embedding associated with that word\n",
    "    \n",
    "#     NOTE: THIS ONLY WORKS BECAUSE WHEN MAKING WORD2INDEX, \n",
    "#     VALUES GUARENTEED TO BE IN ASCENDING ORDER\n",
    "#     '''\n",
    "# #     print(key)\n",
    "#     try:\n",
    "# #         print(index2word[key],model[index2word2[key]].shape)\n",
    "#         pretrained.append(model2[index2word[key]])\n",
    "#     except:\n",
    "#         print(\"Random\")\n",
    "#         pretrained.append(np.random.randn(300))\n",
    "        \n",
    "# pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(2, 3, 4), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH = 20\n",
    "# BATCH_SIZE = 10\n",
    "# KERNEL_SIZES = [5,10,15]\n",
    "# KERNEL_DIM = 100\n",
    "# LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 2\n",
    "KERNEL_SIZES = [3,5,7]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "\n",
    "model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc():\n",
    "    accuracy = 0\n",
    "    for test in test_data:\n",
    "        if test[0].shape[1]>3:# bug I need to fix\n",
    "            pred = model(test[0]).max(1)[1]\n",
    "            pred = pred.data.tolist()[0]\n",
    "            target = test[1].data.tolist()[0][0]\n",
    "            if pred == target:\n",
    "                accuracy += 1\n",
    "\n",
    "\n",
    "    acc = accuracy/len(test_data) * 100\n",
    "#     print(acc)\n",
    "    return acc\n",
    "def f1():\n",
    "    predictions= []\n",
    "    y_test=[]\n",
    "    for i,test in enumerate(getBatch(1, test_data)):\n",
    "        inputs,targets = pad_to_batch(test)\n",
    "#         if test[0].shape[1]>3:# bug I need to fix\n",
    "        predictions.append(model(inputs).max(1)[1].data[0])\n",
    "#         print(targets.data.tolist()[0])\n",
    "        y_test.append(targets.data.tolist()[0])\n",
    "#         print(predictions[-1],'==',y_test[-1])\n",
    "            \n",
    "    f1 = f1_score(y_test, predictions, average=\"weighted\")\n",
    "#     print(\"Naive F1 score: {}\".format(f1))\n",
    "    return f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [enumerate(getBatch(10, test_data))in range(len(test_data))]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s= []\n",
    "accs = []\n",
    "best_model = 0.0\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "#         print(batch)\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "#         print(inputs.shape)\n",
    "        preds = model(inputs, True)\n",
    "        \n",
    "        loss = loss_function(preds, targets)\n",
    "#         print(loss.data[0])\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%1==0:\n",
    "        print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "        losses = []\n",
    "        accs.append(acc())\n",
    "        f1s.append(f1())\n",
    "        print(\"Train Acc: \",accs[-1],\"F1 score on Test Set\",f1s[-1])\n",
    "        if f1s[-1] > best_model:\n",
    "            best_model = f1s[-1]\n",
    "            saveBestModel(model,\"models/Exp1/CNN_Exp1_f1_\"+str(best_model)+\"_acc_\"+str(acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(f1s)),f1s)\n",
    "plt.title(\"F1 Score over 100 Epochs kernels=[3,5,7]\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.savefig(\"F1 Score over Epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accs)),accs)\n",
    "plt.title(\"Train Acc Score over 100 Epochs kernels=[3,5,7]\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc score\")\n",
    "plt.savefig(\"Acc Score over Epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "def saveBestModel(model,filepath):\n",
    "    torch.save(model,filepath)\n",
    "    # test \n",
    "#     m = torch.load(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "CNN Text Classification at higher kernels\n",
    "11,13,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class  CNNClassifier_two_layer(nn.Module):\n",
    "    \n",
    "#     def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(2, 3, 4), dropout=0.5):\n",
    "#         super(CNNClassifier,self).__init__()\n",
    "\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "#         # kernal_size = (K,D) \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         # FC layer 1: 100 dim\n",
    "#         self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, kernel_dim)\n",
    "#         # extra layer\n",
    "#         self.final_layer = nn.Linear(kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "#     def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "#         self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "#         if is_static:\n",
    "#             self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "#     def forward(self, inputs, is_training=False):\n",
    "#         inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "#         inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "#         inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "#         concated = torch.cat(inputs, 1)\n",
    "\n",
    "#         if is_training:\n",
    "#             concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "#         out = self.fc(concated) \n",
    "#         out2 = self.final_layer(out)\n",
    "#         return F.log_softmax(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 2\n",
    "KERNEL_SIZES = [13,15,17]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "\n",
    "model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s= []\n",
    "accs = []\n",
    "best_model=0.0\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "#         print(batch)\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "#         print(inputs.shape)\n",
    "        preds = model(inputs, True)\n",
    "        \n",
    "        loss = loss_function(preds, targets)\n",
    "#         print(loss.data[0])\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%1==0:\n",
    "        print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "        losses = []\n",
    "        accs.append(acc())\n",
    "        f1s.append(f1())\n",
    "        print(\"Train Acc: \",accs[-1],\"F1 score on Test Set\",f1s[-1])\n",
    "        if f1s[-1] > best_model:\n",
    "            best_model = f1s[-1]\n",
    "            saveBestModel(model,\"models/Exp2/CNN_Exp2_f1_\"+str(best_model)+\"_acc_\"+str(accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(f1s)),f1s)\n",
    "plt.title(\"F1 Score over 100 Epochs kernels=[13,15,17]\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.savefig(\"F1 Score over Epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accs)),accs)\n",
    "plt.title(\"Trin Acc Score over 100 Epochs kernels=[13,15,17]\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc score\")\n",
    "plt.savefig(\"Acc Score over Epochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 3:\n",
    "Increase Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 2\n",
    "KERNEL_SIZES = [5,7,9]\n",
    "KERNEL_DIM = 200\n",
    "LR = 0.001\n",
    "\n",
    "model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s= []\n",
    "accs = []\n",
    "best_model=0.0\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "#         print(batch)\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "#         print(inputs.shape)\n",
    "        preds = model(inputs, True)\n",
    "        \n",
    "        loss = loss_function(preds, targets)\n",
    "#         print(loss.data[0])\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%1==0:\n",
    "        print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "        losses = []\n",
    "        accs.append(acc())\n",
    "        f1s.append(f1())\n",
    "        print(\"Train Acc: \",accs[-1],\"F1 score on Test Set\",f1s[-1])\n",
    "        if f1s[-1] > best_model:\n",
    "            best_model = f1s[-1]\n",
    "            saveBestModel(model,\"models/Exp3/CNN_Exp3_f1_\"+str(best_model)+\"_acc_\"+str(accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(f1s)),f1s)\n",
    "plt.title(\"F1 Score over 100 Epochs kernels=[5,7,9], Kernel Dim 200\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.savefig(\"F1 Score over 100 Epochs kernels=[5,7,9], Kernel Dim 200.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accs)),accs)\n",
    "plt.title(\"Train Acc Score over 100 Epochs kernels=[5,7,9], Kernel Dim 200\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Acc score\")\n",
    "plt.savefig(\"F1 Score over 100 Epochs kernels=[5,7,9], Kernel Dim 200.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join([i.text for i in list(train_corp[0].tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_corp[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(train_corp[0].text,word_count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
