{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import textacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# Splitting\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# ast.literal_eval\n",
    "# df = get_undersampled(pd.read_csv(\"data/train_raw.csv\"))\n",
    "# df2 = get_undersampled(pd.read_csv(\"data/test_raw.csv\"))\n",
    "\n",
    "df = pd.read_csv(\"data/train_raw.csv\")\n",
    "df2 = pd.read_csv(\"data/test_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags =df[\"tags\"]\n",
    "test_tags = df2[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df\n",
    "# labels.head()\n",
    "test_labels = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tags.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MacArthur grant',\n",
       " 'activism',\n",
       " 'business',\n",
       " 'cities',\n",
       " 'environment',\n",
       " 'green',\n",
       " 'inequality',\n",
       " 'politics',\n",
       " 'pollution']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(tags[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "train_corp = textacy.Corpus(lang='en')\n",
    "for l in train_tags.tolist():\n",
    "    train_corp.add_text(' '.join(ast.literal_eval(l)))\n",
    "test_corp = textacy.Corpus(lang='en')\n",
    "for l in test_tags.tolist():\n",
    "    test_corp.add_text(' '.join(ast.literal_eval(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entire corpus\n",
    "TR_LEN = len(train_corp)\n",
    "TE_LEN =len(test_corp)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire_corpus = textacy.Corpus(lang='en')\n",
    "# for l in train_tags.tolist()+test_tags.tolist():\n",
    "#     entire_corpus.add_text(' '.join(ast.literal_eval(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1207"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entire_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus(590 docs; 5196 tokens)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data - Without combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_idf_corpus(corpus):\n",
    "#     vectorizer = textacy.Vectorizer(\n",
    "#         tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "#         min_df=2, max_df=0.95)\n",
    "#     doc_term_matrix = vectorizer.fit_transform(\n",
    "#         (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "#          for doc in corpus))\n",
    "#     return doc_term_matrix\n",
    "# X = tf_idf_corpus(train_corp)\n",
    "# X_test = tf_idf_corpus(test_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize data with combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_corpus(train_c,test_c):\n",
    "    vectorizer = textacy.Vectorizer(\n",
    "        tf_type='linear', apply_idf=True, idf_type='smooth', norm='l2',\n",
    "        min_df=2, max_df=0.95)\n",
    "    train = vectorizer.fit_transform(\n",
    "        (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "         for doc in train_c))\n",
    "    test = vectorizer.transform(\n",
    "        (doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)\n",
    "         for doc in test_c))\n",
    "    return train,test\n",
    "X_train,X_test = tf_idf_corpus(train_corp,test_corp)\n",
    "#  = tf_idf_corpus(test)\n",
    "# X_train=X[:TR_LEN]\n",
    "# X_test = X[TR_LEN:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1850, 472) (1850,) (617, 472) (617,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logreg_results(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training Logistic Regression\")\n",
    "    clf = LogisticRegression()\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    scores = []\n",
    "    f = 0\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_tr, X_t = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_t = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        predictions = clf.predict(X_t)\n",
    "        scores.append(f1_score(y_t, predictions, average=\"weighted\"))\n",
    "        print(\"Fold {}: {}\".format(f+1, scores[-1]))\n",
    "        f+=1\n",
    "    print(\"Logistic cross-validation F1: {}\".format(np.mean(scores)))\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Logistic F1 on the test set: {}\".format(f1_score(y_test, clf.predict(X_test),average=\"weighted\")))\n",
    "    \n",
    "def get_svm_results(X_train, X_test, y_train, y_test):\n",
    "    print(\"Training SVM\")\n",
    "\n",
    "    clf = SVC(kernel='linear')\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    scores = []\n",
    "    f = 0\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_tr, X_t = X_train[train_index], X_train[test_index]\n",
    "        y_tr, y_t = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        predictions = clf.predict(X_t)\n",
    "        scores.append(f1_score(y_t, predictions, average=\"weighted\"))\n",
    "        print(\"Fold {}: {}\".format(f+1, scores[-1]))\n",
    "        f+=1\n",
    "    print(\"SVM cross-validation f1: {}\".format(np.mean(scores)))\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"SVM F1 on the test set: {}\".format(f1_score(y_test, clf.predict(X_test),average=\"weighted\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels[\"label\"].as_matrix()\n",
    "y_test = test_labels[\"label\"].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_logreg_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_svm_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression\n",
      "Fold 1: 0.4751060259318066\n",
      "Fold 2: 0.48602054919290627\n",
      "Fold 3: 0.451324947053127\n",
      "Fold 4: 0.4676515270363124\n",
      "Fold 5: 0.4629097148250837\n",
      "Logistic cross-validation F1: 0.46860255280784724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic F1 on the test set: 0.48355107887293675\n"
     ]
    }
   ],
   "source": [
    "get_logreg_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.4672143367484917\n",
      "Fold 2: 0.4535639774465551\n",
      "Fold 3: 0.4619054624515376\n",
      "Fold 4: 0.48608534685545735\n",
      "Fold 5: 0.4788040905013588\n",
      "SVM cross-validation f1: 0.4695146428006801\n",
      "SVM F1 on the test set: 0.4777761617924067\n"
     ]
    }
   ],
   "source": [
    "get_svm_results(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_undersampled(df, threshold=200, window=10):\n",
    "        indices = df['label'].value_counts()[df['label'].value_counts() > threshold].index.tolist()\n",
    "\n",
    "        for ind in indices:\n",
    "            over_df = df[df['label'] == ind].reset_index(drop=True)\n",
    "            df = df.drop(df[df['label'] == ind].index)\n",
    "            to_drop = np.random.randint(threshold-window, \n",
    "                                        threshold+window)\n",
    "\n",
    "            trans_ids = np.random.choice(range(len(over_df)),\n",
    "                                         to_drop)\n",
    "\n",
    "            dfs_to_add = over_df.iloc[trans_ids]\n",
    "            df = pd.concat([df, dfs_to_add]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        # Shuffle result\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Text Classification, no undersamplng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp train_5500.label train_5500.label.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "def flatten2(l):\n",
    "    res = []\n",
    "    for elem in l:\n",
    "        for item in elem.split(' '):\n",
    "#             a.append(item)\n",
    "            res.append(item)\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('train_5500.label.txt', 'r', encoding='latin-1').readlines()\n",
    "\n",
    "data = [[d.split(':')[1][:-1], d.split(':')[0]] for d in data]\n",
    "\n",
    "X_2, y_2 = list(zip(*data))\n",
    "vocab = list(set(flatten2(X_2)))#converts sentences into sequence of characters, and finds unique set of characters\n",
    "X_2 = [i.split(' ') for i in X_2 ]\n",
    "### Num masking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(X_2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(set(flatten(X_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "[a.update(entire_corpus[i].to_bag_of_terms(ngrams=1, as_strings=True))  for i in range(len(entire_corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entertainment': 1,\n",
       " 'global': 1,\n",
       " 'issue': 1,\n",
       " 'live': 1,\n",
       " 'music': 2,\n",
       " 'performance': 1,\n",
       " 'poetry': 1,\n",
       " 'politic': 1,\n",
       " 'war': 1,\n",
       " 'art': 2,\n",
       " 'book': 1,\n",
       " 'chemistry': 1,\n",
       " 'food': 1,\n",
       " 'life': 1,\n",
       " 'love': 1,\n",
       " 'science': 1,\n",
       " 'gender': 1,\n",
       " 'equality': 1,\n",
       " 'activism': 1,\n",
       " 'feminism': 1,\n",
       " 'government': 1,\n",
       " 'history': 1,\n",
       " 'inequality': 1,\n",
       " 'motivation': 1,\n",
       " 'personal': 1,\n",
       " 'growth': 1,\n",
       " 'social': 1,\n",
       " 'change': 1,\n",
       " 'society': 1,\n",
       " 'woman': 1,\n",
       " 'tedx': 1,\n",
       " 'creativity': 1,\n",
       " 'capitalism': 1,\n",
       " 'economic': 1,\n",
       " 'money': 1,\n",
       " 'policy': 1,\n",
       " 'resource': 1,\n",
       " 'work': 1,\n",
       " 'community': 1,\n",
       " 'humanity': 1,\n",
       " 'identity': 1,\n",
       " 'race': 1,\n",
       " 'violence': 1,\n",
       " 'animal': 1,\n",
       " 'nature': 1,\n",
       " 'photography': 1,\n",
       " 'film': 1,\n",
       " 'design': 1,\n",
       " 'humor': 1,\n",
       " 'philosophy': 1,\n",
       " 'storytelling': 1,\n",
       " 'collaboration': 1,\n",
       " 'crowdsourcing': 1,\n",
       " 'datum': 1,\n",
       " 'technology': 1,\n",
       " 'visualization': 1,\n",
       " 'comedy': 1,\n",
       " 'brain': 1,\n",
       " 'presentation': 1,\n",
       " 'ai': 1,\n",
       " 'drone': 1,\n",
       " 'robot': 1,\n",
       " 'happiness': 1,\n",
       " 'psychology': 1,\n",
       " 'relationship': 1,\n",
       " 'storytell': 1,\n",
       " 'cancer': 1,\n",
       " 'health': 1,\n",
       " 'care': 1,\n",
       " 'medicine': 1,\n",
       " 'business': 1,\n",
       " 'culture': 1,\n",
       " 'introvert': 1,\n",
       " 'middle east': 1,\n",
       " 'middle': 1,\n",
       " 'east': 1,\n",
       " 'ted': 1,\n",
       " 'fellows': 1,\n",
       " 'terrorism': 1,\n",
       " 'tednyc': 1,\n",
       " 'education': 1,\n",
       " 'library': 1,\n",
       " 'biology': 1,\n",
       " 'nanoscale': 1,\n",
       " 'nasa': 1,\n",
       " 'planets': 1,\n",
       " 'exploration': 1,\n",
       " 'solar': 1,\n",
       " 'system': 1,\n",
       " 'space': 1,\n",
       " 'universe': 1,\n",
       " 'city': 1,\n",
       " 'europe united states': 1,\n",
       " 'europe': 1,\n",
       " 'united': 1,\n",
       " 'states': 1,\n",
       " 'big': 1,\n",
       " 'problem': 1,\n",
       " 'democracy': 1,\n",
       " 'finance': 1,\n",
       " 'investment': 1,\n",
       " 'leadership': 1,\n",
       " 'dance': 1,\n",
       " 'genetic': 1,\n",
       " 'violin': 1,\n",
       " 'cognitive': 1,\n",
       " 'communication': 1,\n",
       " 'medium': 1,\n",
       " 'ted brain trust': 1,\n",
       " 'trust': 1,\n",
       " 'archaeology': 1,\n",
       " 'dinosaur': 1,\n",
       " 'fish': 1,\n",
       " 'poverty': 1,\n",
       " 'behavioral': 1,\n",
       " 'energy': 1,\n",
       " 'biodiversity': 1,\n",
       " 'biotech': 1,\n",
       " 'alternative': 1,\n",
       " 'disease': 1,\n",
       " 'heart': 1,\n",
       " 'illness': 1,\n",
       " 'medical': 1,\n",
       " 'image': 1,\n",
       " 'research': 1,\n",
       " 'physiology': 1,\n",
       " 'public': 1,\n",
       " 'prize': 1,\n",
       " 'code': 1,\n",
       " 'literature': 1,\n",
       " 'writing': 1,\n",
       " 'mission': 1,\n",
       " 'blue': 1,\n",
       " 'ocean': 1,\n",
       " 'singer': 1,\n",
       " 'security': 1,\n",
       " 'charter': 1,\n",
       " 'compassion': 1,\n",
       " 'faith': 1,\n",
       " 'religion': 1,\n",
       " 'internet': 1,\n",
       " 'future': 1,\n",
       " 'law': 1,\n",
       " 'open': 1,\n",
       " 'source': 1,\n",
       " 'software': 1,\n",
       " 'climate': 1,\n",
       " 'news': 1,\n",
       " 'engineer': 1,\n",
       " 'innovation': 1,\n",
       " 'neuroscience': 1,\n",
       " 'physics': 1,\n",
       " 'sound': 1,\n",
       " 'child': 1,\n",
       " 'entrepreneur': 1,\n",
       " 'gaming': 1,\n",
       " 'toy': 1,\n",
       " 'age': 1,\n",
       " 'ptsd': 1,\n",
       " 'tedmed': 1,\n",
       " 'depression': 1,\n",
       " 'mental': 1,\n",
       " 'mind': 1,\n",
       " 'pain': 1,\n",
       " 'choice': 1,\n",
       " 'fear': 1,\n",
       " 'journalism': 1,\n",
       " 'computer': 1,\n",
       " 'development': 1,\n",
       " 'immigration': 1,\n",
       " 'refuge': 1,\n",
       " 'data': 1,\n",
       " 'sociology': 1,\n",
       " 'buddhism': 1,\n",
       " 'wunderkind': 1,\n",
       " 'astronomy': 1,\n",
       " 'dark': 1,\n",
       " 'matter': 1,\n",
       " 'telescope': 1,\n",
       " 'invention': 1,\n",
       " 'macarthur': 1,\n",
       " 'grant': 1,\n",
       " 'material': 1,\n",
       " 'product': 1,\n",
       " 'vaccine': 1,\n",
       " 'agriculture': 1,\n",
       " 'biomechanic': 1,\n",
       " 'botany': 1,\n",
       " 'ebola': 1,\n",
       " 'ecology': 1,\n",
       " 'engineering': 1,\n",
       " 'environment': 1,\n",
       " 'garden': 1,\n",
       " 'green': 1,\n",
       " 'potential': 1,\n",
       " 'sustainability': 1,\n",
       " 'virus': 1,\n",
       " 'water': 1,\n",
       " 'infrastructure': 1,\n",
       " 'new york': 1,\n",
       " 'new': 1,\n",
       " 'york': 1,\n",
       " 'asia': 1,\n",
       " 'adventure': 1,\n",
       " 'parenting': 1,\n",
       " 'surveillance': 1,\n",
       " 'crime': 1,\n",
       " 'architecture': 1,\n",
       " 'bacteria': 1,\n",
       " 'biosphere': 1,\n",
       " 'death': 1,\n",
       " 'microbe': 1,\n",
       " 'demo': 1,\n",
       " 'africa': 1,\n",
       " 'aids': 1,\n",
       " 'microbiology': 1,\n",
       " 'algorithm': 1,\n",
       " 'sight': 1,\n",
       " 'productivity': 1,\n",
       " 'vocal': 1,\n",
       " 'hack': 1,\n",
       " 'sanitation': 1,\n",
       " 'statistic': 1,\n",
       " 'sens': 1,\n",
       " 'narcotic': 1,\n",
       " 'peace': 1,\n",
       " 'youth': 1,\n",
       " 'sex': 1,\n",
       " 'privacy': 1,\n",
       " 'web': 1,\n",
       " 'microsoft': 1,\n",
       " 'virtual': 1,\n",
       " 'reality': 1,\n",
       " 'cosmo': 1,\n",
       " 'anthropocene': 1,\n",
       " 'ancient': 1,\n",
       " 'world': 1,\n",
       " 'pollution': 1,\n",
       " 'disaster': 1,\n",
       " 'relief': 1,\n",
       " 'oil': 1,\n",
       " 'dna': 1,\n",
       " 'human': 1,\n",
       " 'body': 1,\n",
       " 'hiv': 1,\n",
       " 'parent': 1,\n",
       " 'marketing': 1,\n",
       " 'anthropology': 1,\n",
       " 'car': 2,\n",
       " 'driverless': 1,\n",
       " 'natural': 1,\n",
       " 'nuclear': 1,\n",
       " 'telecom': 1,\n",
       " 'extraterrestrial': 1,\n",
       " 'map': 1,\n",
       " 'theater': 1,\n",
       " 'sport': 1,\n",
       " 'ted fellows': 1,\n",
       " 'family': 1,\n",
       " 'brazil': 1,\n",
       " 'industrial': 1,\n",
       " 'philanthropy': 1,\n",
       " 'play': 1,\n",
       " 'speak': 1,\n",
       " 'word': 1,\n",
       " 'museum': 1,\n",
       " 'novel': 1,\n",
       " 'egypt': 1,\n",
       " '3d': 1,\n",
       " 'print': 1,\n",
       " 'manufacture': 1,\n",
       " 'molecular': 1,\n",
       " 'pharmaceutical': 1,\n",
       " 'synthetic': 1,\n",
       " 'consciousness': 1,\n",
       " 'evolution': 1,\n",
       " 'advertise': 1,\n",
       " 'string': 1,\n",
       " 'theory': 1,\n",
       " 'slavery': 1,\n",
       " 'trafficking': 1,\n",
       " 'corruption': 1,\n",
       " 'animation': 1,\n",
       " 'personality': 1,\n",
       " 'success': 1,\n",
       " 'aircraft': 1,\n",
       " 'interview': 1,\n",
       " 'state': 1,\n",
       " 'build': 1,\n",
       " 'language': 1,\n",
       " 'write': 1,\n",
       " 'guitar': 1,\n",
       " 'interface': 1,\n",
       " 'online': 1,\n",
       " 'video': 1,\n",
       " 'tedyouth': 1,\n",
       " 'fashion': 1,\n",
       " 'manufacturing': 1,\n",
       " 'shopping': 1,\n",
       " 'nonviolence': 1,\n",
       " 'obesity': 1,\n",
       " 'protest': 1,\n",
       " 'smell': 1,\n",
       " 'europe surveillance': 1,\n",
       " 'intelligence': 1,\n",
       " 'illusion': 1,\n",
       " 'magic': 1,\n",
       " 'alzheimer': 1,\n",
       " \"'s\": 1,\n",
       " 'consumerism': 1,\n",
       " 'transportation': 1,\n",
       " 'self': 1,\n",
       " 'ted book': 1,\n",
       " 'imaging': 1,\n",
       " 'beauty': 1,\n",
       " 'math': 1,\n",
       " 'india': 1,\n",
       " 'addiction': 1,\n",
       " 'lgbt': 1,\n",
       " 'teaching': 1,\n",
       " 'gun': 1,\n",
       " 'military': 1,\n",
       " 'friendship': 1,\n",
       " 'africa asia': 1,\n",
       " 'africa slavery': 1,\n",
       " 'prosthetic': 1,\n",
       " 'tree': 1,\n",
       " 'weather': 1,\n",
       " 'criminal': 1,\n",
       " 'justice': 1,\n",
       " 'prison': 1,\n",
       " 'population': 1,\n",
       " 'biomimicry': 1,\n",
       " 'failure': 1,\n",
       " 'bioethic': 1,\n",
       " 'epidemiology': 1,\n",
       " 'disability': 1,\n",
       " 'machine': 1,\n",
       " 'learn': 1,\n",
       " 'iran': 1,\n",
       " 'iraq': 1,\n",
       " 'urban': 1,\n",
       " 'planning': 1,\n",
       " 'africa egypt': 1,\n",
       " 'bird': 1,\n",
       " 'conservation': 1,\n",
       " 'wind': 1,\n",
       " 'movie': 1,\n",
       " 'decision': 1,\n",
       " 'make': 1,\n",
       " 'man': 1,\n",
       " 'sexual': 1,\n",
       " 'china': 1,\n",
       " 'programming': 1,\n",
       " 'blindness': 1,\n",
       " 'prediction': 1,\n",
       " 'suicide': 1,\n",
       " 'evolutionary': 1,\n",
       " 'debate': 1,\n",
       " 'deextinction': 1,\n",
       " 'en': 1,\n",
       " 'espa√±ol': 1,\n",
       " 'curiosity': 1,\n",
       " 'discovery': 1,\n",
       " 'pandemic': 1,\n",
       " 'vulnerability': 1,\n",
       " 'extreme': 1,\n",
       " 'flight': 1,\n",
       " 'bioethics': 1,\n",
       " 'africa asia google': 1,\n",
       " 'google': 1,\n",
       " 'goal': 1,\n",
       " 'set': 1,\n",
       " 'mindfulness': 1,\n",
       " 'teach': 1,\n",
       " 'balance': 1,\n",
       " 'conduct': 1,\n",
       " 'mars nasa': 1,\n",
       " 'mars': 1,\n",
       " 'bee': 1,\n",
       " 'mars planets': 1,\n",
       " 'insect': 1,\n",
       " 'plant': 1,\n",
       " 'student': 1,\n",
       " 'nobel': 1,\n",
       " 'surgery': 1,\n",
       " 'speech': 1,\n",
       " 'river': 1,\n",
       " 'origami': 1,\n",
       " 'painting': 1,\n",
       " 'empathy': 1,\n",
       " 'shop': 1,\n",
       " 'complexity': 1,\n",
       " 'time': 1,\n",
       " 'wikipedia': 1,\n",
       " 'travel': 1,\n",
       " 'islam middle east': 1,\n",
       " 'islam': 1,\n",
       " 'farm': 1,\n",
       " 'origin': 1,\n",
       " 'crowdsourc': 1,\n",
       " 'united states': 1,\n",
       " 'television': 1,\n",
       " 'typography': 1,\n",
       " 't': 1,\n",
       " 'morality': 1,\n",
       " 'debate guns': 1,\n",
       " 'guns': 1,\n",
       " 'monkey': 1,\n",
       " 'piano': 1,\n",
       " 'foreign': 1,\n",
       " 'microfinance': 1,\n",
       " 'mining': 1,\n",
       " 'europe middle east': 1,\n",
       " 'movi': 1,\n",
       " 'ted prize': 1,\n",
       " 'anthropocene natural': 1,\n",
       " 'cyborg': 1,\n",
       " 'exoskeleton': 1,\n",
       " 'primate': 1,\n",
       " 'africa asia europe foreign policy iran': 1,\n",
       " 'middle east new york south america': 1,\n",
       " 'syria': 1,\n",
       " 'south': 1,\n",
       " 'america': 1,\n",
       " 'crispr': 1,\n",
       " 'bully': 1,\n",
       " 'evil': 1,\n",
       " 'middle east syria': 1,\n",
       " 'refugee': 1,\n",
       " 'building': 1,\n",
       " 'planet': 1,\n",
       " 'astrobiology': 1,\n",
       " 'plastic': 1,\n",
       " 'paleontology': 1,\n",
       " 'memory': 1,\n",
       " 'brand': 1,\n",
       " 'blindness senses': 1,\n",
       " 'senses': 1,\n",
       " 'simplicity': 1,\n",
       " 'traffic': 1,\n",
       " 'asteroid': 1,\n",
       " 'geology': 1,\n",
       " 'asia united states': 1,\n",
       " 'bioethics south america': 1,\n",
       " 'compose': 1,\n",
       " 'god': 1,\n",
       " 'fellow': 1,\n",
       " 'grammar': 1,\n",
       " 'pregnancy': 1,\n",
       " 'south america': 1,\n",
       " 'glacier': 1,\n",
       " 'christianity': 1,\n",
       " 'funny': 1,\n",
       " 'meditation': 1,\n",
       " 'bang': 1}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vocab of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={'<PAD>': 0, '<UNK>': 1}\n",
    "for key in a.keys():\n",
    "    if key not in word2index:\n",
    "        word2index[key] = len(word2index)\n",
    "index2word = {v:k for k, v in word2index.items()}#loop through sets, and set the value as index, and key as value\n",
    "\n",
    "word2index2={'<PAD>': 0, '<UNK>': 1}\n",
    "for key in vocab:\n",
    "    if key not in word2index2:\n",
    "        word2index2[key] = len(word2index2)\n",
    "index2word2 = {v:k for k, v in word2index2.items()}#loop through sets, and set the value as index, and key as value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Label Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2index = {}\n",
    "\n",
    "for cl in set(labels):\n",
    "    if target2index.get(cl) is None:\n",
    "        target2index[cl] = len(target2index)# assign an index to unique labels\n",
    "\n",
    "index2target = {v:k for k, v in target2index.items()}\n",
    "\n",
    "\n",
    "target2index2 = {}\n",
    "\n",
    "for cl in set(y_2):\n",
    "    if target2index2.get(cl) is None:\n",
    "        target2index2[cl] = len(target2index2)# assign an index to unique labels\n",
    "\n",
    "index2target2 = {v:k for k, v in target2index2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'baseball': 2,\n",
       " 'Buda': 3,\n",
       " 'Wines': 4,\n",
       " 'year': 5,\n",
       " 'named': 6,\n",
       " 'Faber': 7,\n",
       " 'Milton': 8,\n",
       " 'horsepower': 9,\n",
       " 'Biscay': 10,\n",
       " 'khaki': 11,\n",
       " 'dingoes': 12,\n",
       " 'positions': 13,\n",
       " 'Wassermann': 14,\n",
       " 'veins': 15,\n",
       " 'mechanism': 16,\n",
       " 'Holy': 17,\n",
       " 'Lawrence': 18,\n",
       " 'Danube': 19,\n",
       " 'Yousuf': 20,\n",
       " 'Craig': 21,\n",
       " 'birth': 22,\n",
       " 'A&W': 23,\n",
       " 'Methodist': 24,\n",
       " 'games': 25,\n",
       " 'Dennis': 26,\n",
       " 'coastal': 27,\n",
       " 'Springs': 28,\n",
       " '!': 29,\n",
       " 'Spider-Man': 30,\n",
       " 'build': 31,\n",
       " 'fastest': 32,\n",
       " 'citizen': 33,\n",
       " 'are': 34,\n",
       " 'Zealand': 35,\n",
       " 'Metropolis': 36,\n",
       " 'Internet': 37,\n",
       " 'abolished': 38,\n",
       " 'ouzo': 39,\n",
       " 'pressured': 40,\n",
       " 'begins': 41,\n",
       " 'baseemen': 42,\n",
       " 'Pacific': 43,\n",
       " 'softball': 44,\n",
       " 'Philadelphia': 45,\n",
       " 'Somalia': 46,\n",
       " 'morning': 47,\n",
       " 'Reflections': 48,\n",
       " 'cop': 49,\n",
       " 'operate': 50,\n",
       " 'Nabokov': 51,\n",
       " 'monkey': 52,\n",
       " 'Khrushchev': 53,\n",
       " 'Tristan': 54,\n",
       " 'spend': 55,\n",
       " 'terms': 56,\n",
       " 'enhance': 57,\n",
       " 'somene': 58,\n",
       " 'Genome': 59,\n",
       " 'rain': 60,\n",
       " 'describing': 61,\n",
       " 'control': 62,\n",
       " 'Osbourne': 63,\n",
       " 'fall': 64,\n",
       " 'Huxley': 65,\n",
       " 'rocks': 66,\n",
       " 'handful': 67,\n",
       " 'Minnesota': 68,\n",
       " 'Court': 69,\n",
       " 'governmental': 70,\n",
       " 'Give': 71,\n",
       " 'migrates': 72,\n",
       " 'except': 73,\n",
       " 'Joe': 74,\n",
       " 'Falklands': 75,\n",
       " 'Leon': 76,\n",
       " 'Twin': 77,\n",
       " 'abbreviate': 78,\n",
       " 'wop': 79,\n",
       " 'Illinois': 80,\n",
       " 'disposable': 81,\n",
       " 'opponents': 82,\n",
       " 'honorary': 83,\n",
       " '25th': 84,\n",
       " 'spread': 85,\n",
       " 'turret': 86,\n",
       " 'enters': 87,\n",
       " 'Panther': 88,\n",
       " 'everything': 89,\n",
       " 'hundred': 90,\n",
       " 'label': 91,\n",
       " 'granite': 92,\n",
       " 'alternator': 93,\n",
       " 'dogtown': 94,\n",
       " 'deployed': 95,\n",
       " 'kangaroo': 96,\n",
       " 'zitoni': 97,\n",
       " 'Corner': 98,\n",
       " 'Nike': 99,\n",
       " 'queries': 100,\n",
       " 'Chiefs': 101,\n",
       " 'accident': 102,\n",
       " 'cow': 103,\n",
       " 'ripening': 104,\n",
       " 'banking': 105,\n",
       " 'SIC': 106,\n",
       " 'multiplexer': 107,\n",
       " 'amount': 108,\n",
       " 'tokens': 109,\n",
       " 'heat': 110,\n",
       " 'calendar': 111,\n",
       " 'submarine': 112,\n",
       " 'Kemper': 113,\n",
       " 'Renaissance': 114,\n",
       " 'remake': 115,\n",
       " 'consisting': 116,\n",
       " 'ballots': 117,\n",
       " 'celebrated': 118,\n",
       " 'bullfighting': 119,\n",
       " 'Houdini': 120,\n",
       " 'CNN': 121,\n",
       " 'Carolina': 122,\n",
       " 'golfing': 123,\n",
       " 'society': 124,\n",
       " 'daily': 125,\n",
       " 'physician': 126,\n",
       " 'novel': 127,\n",
       " 'werewolf': 128,\n",
       " 'removed': 129,\n",
       " 'badly': 130,\n",
       " '1959': 131,\n",
       " 'Oz': 132,\n",
       " 'engines': 133,\n",
       " 'attempts': 134,\n",
       " 'Brandt': 135,\n",
       " 'regarding': 136,\n",
       " 'Best': 137,\n",
       " 'wearing': 138,\n",
       " 'sculptress': 139,\n",
       " 'Hirsch': 140,\n",
       " 'influences': 141,\n",
       " 'expelled': 142,\n",
       " 'parking': 143,\n",
       " 'orthopedics': 144,\n",
       " 'hump': 145,\n",
       " 'Hampshire': 146,\n",
       " 'Palmer': 147,\n",
       " 'throw': 148,\n",
       " 'Cool': 149,\n",
       " 'will': 150,\n",
       " 'bar-code': 151,\n",
       " 'Shevardnadze': 152,\n",
       " 'Churchill': 153,\n",
       " 'Hot': 154,\n",
       " 'estuary': 155,\n",
       " 'BIOS': 156,\n",
       " 'lady': 157,\n",
       " 'slane': 158,\n",
       " 'carpal': 159,\n",
       " 'spices': 160,\n",
       " 'Cribbage': 161,\n",
       " 'three-letter': 162,\n",
       " 'Qatar': 163,\n",
       " 'mailman': 164,\n",
       " 'Trail': 165,\n",
       " 'collier': 166,\n",
       " 'rules': 167,\n",
       " 'orca': 168,\n",
       " 'lunch': 169,\n",
       " 'stripe': 170,\n",
       " 'competition': 171,\n",
       " 'held': 172,\n",
       " 'women': 173,\n",
       " 'Pan': 174,\n",
       " 'expedition': 175,\n",
       " 'era': 176,\n",
       " 'Judson': 177,\n",
       " 'donated': 178,\n",
       " 'Caulfield': 179,\n",
       " 'Andie': 180,\n",
       " 'Barbie': 181,\n",
       " 'alive': 182,\n",
       " 'leads': 183,\n",
       " 'helicopter': 184,\n",
       " 'giving': 185,\n",
       " 'character': 186,\n",
       " 'Goering': 187,\n",
       " 'Christi': 188,\n",
       " 'woke': 189,\n",
       " 'Stones': 190,\n",
       " 'course': 191,\n",
       " 'Mozambique': 192,\n",
       " 'pushy': 193,\n",
       " 'Greek': 194,\n",
       " 'someone': 195,\n",
       " 'Atlantic': 196,\n",
       " 'Flying': 197,\n",
       " 'Sergeant': 198,\n",
       " 'corporate': 199,\n",
       " 'eyes': 200,\n",
       " 'probability': 201,\n",
       " 'trademark': 202,\n",
       " 'Ninety-Five': 203,\n",
       " 'calculator': 204,\n",
       " 'Rhett': 205,\n",
       " 'Leoncavallo': 206,\n",
       " 'Theodore': 207,\n",
       " 'House': 208,\n",
       " 'learned': 209,\n",
       " 'barroom': 210,\n",
       " 'Purina': 211,\n",
       " 'Cobol': 212,\n",
       " 'Blood': 213,\n",
       " 'lives': 214,\n",
       " 'justice': 215,\n",
       " 'Garvey': 216,\n",
       " 'Barnum': 217,\n",
       " 'slits': 218,\n",
       " 'Sons': 219,\n",
       " '327': 220,\n",
       " 'fight': 221,\n",
       " 'attractions': 222,\n",
       " 'Bunyan': 223,\n",
       " 'Astroturf': 224,\n",
       " 'Drinks': 225,\n",
       " 'Harold': 226,\n",
       " 'jockey': 227,\n",
       " 'CBS': 228,\n",
       " 'announce': 229,\n",
       " 'commander': 230,\n",
       " 'buffalo': 231,\n",
       " 'Journal': 232,\n",
       " 'chloroplasts': 233,\n",
       " 'Spaghetti-o': 234,\n",
       " 'history': 235,\n",
       " 'vats': 236,\n",
       " 'left-handed': 237,\n",
       " 'adoptive': 238,\n",
       " '1985': 239,\n",
       " 'batteries': 240,\n",
       " 'days': 241,\n",
       " 'statement': 242,\n",
       " 'Bugs': 243,\n",
       " 'Bank': 244,\n",
       " '1977': 245,\n",
       " 'teaspoons': 246,\n",
       " 'strips': 247,\n",
       " 'Six': 248,\n",
       " 'coney': 249,\n",
       " 'Bars': 250,\n",
       " 'commanders': 251,\n",
       " 'darts': 252,\n",
       " 'Euchre': 253,\n",
       " 'Turks': 254,\n",
       " 'Knute': 255,\n",
       " 'majority': 256,\n",
       " 'out': 257,\n",
       " 'Frankie': 258,\n",
       " 'followed': 259,\n",
       " \"'The\": 260,\n",
       " 'bones': 261,\n",
       " 'Wall': 262,\n",
       " 'Arnold': 263,\n",
       " 'breeding': 264,\n",
       " 'transcript': 265,\n",
       " 'small': 266,\n",
       " 'Daylight': 267,\n",
       " 'department': 268,\n",
       " 'includes': 269,\n",
       " 'Unsafe': 270,\n",
       " 'stations': 271,\n",
       " 'star-faring': 272,\n",
       " 'saw': 273,\n",
       " 'embassy': 274,\n",
       " 'President-to-be': 275,\n",
       " 'Lincolns': 276,\n",
       " 'Grandma': 277,\n",
       " 'decade': 278,\n",
       " 'section': 279,\n",
       " 'touted': 280,\n",
       " 'popularly': 281,\n",
       " 'Breony': 282,\n",
       " 'household': 283,\n",
       " 'Brave': 284,\n",
       " 'Andrea': 285,\n",
       " 'knife': 286,\n",
       " 'Jews': 287,\n",
       " 'lo-o-ove': 288,\n",
       " 'Keaton': 289,\n",
       " 'Rossetti': 290,\n",
       " 'Clinton': 291,\n",
       " 'bigger': 292,\n",
       " 'man': 293,\n",
       " 'sprouts': 294,\n",
       " 'villainous': 295,\n",
       " 'lead': 296,\n",
       " 'Houston': 297,\n",
       " 'Forest': 298,\n",
       " 'slave': 299,\n",
       " 'gr': 300,\n",
       " 'gladiator': 301,\n",
       " 'From': 302,\n",
       " 'Representatives': 303,\n",
       " 'hold': 304,\n",
       " 'blunder': 305,\n",
       " 'minded': 306,\n",
       " 'Channel': 307,\n",
       " 'Horse': 308,\n",
       " 'prayer': 309,\n",
       " 'virtues': 310,\n",
       " 'look': 311,\n",
       " 'Arrow': 312,\n",
       " 'Hawaiian': 313,\n",
       " 'membership': 314,\n",
       " 'jeans': 315,\n",
       " 'shores': 316,\n",
       " 'chiropodist': 317,\n",
       " 'Bohannon': 318,\n",
       " 'nursery': 319,\n",
       " '196': 320,\n",
       " '1943': 321,\n",
       " 'Holland': 322,\n",
       " 'microcontrollers': 323,\n",
       " 'Guam': 324,\n",
       " 'Boy': 325,\n",
       " 'Youth': 326,\n",
       " 'army': 327,\n",
       " 'tribes': 328,\n",
       " 'principle': 329,\n",
       " 'begin': 330,\n",
       " 'equation': 331,\n",
       " 'S&P': 332,\n",
       " 'Minneapolis': 333,\n",
       " 'pins': 334,\n",
       " 'flow': 335,\n",
       " 'panic': 336,\n",
       " 'Trans': 337,\n",
       " '1915': 338,\n",
       " 'IP': 339,\n",
       " 'Indira': 340,\n",
       " 'forehead': 341,\n",
       " 'Bouvier': 342,\n",
       " 'tax': 343,\n",
       " 'Shalom': 344,\n",
       " 'computer': 345,\n",
       " 'commercially': 346,\n",
       " 'sartorial': 347,\n",
       " 'projects': 348,\n",
       " 'How': 349,\n",
       " 'sweaty': 350,\n",
       " 'Pizarro': 351,\n",
       " 'touching': 352,\n",
       " 'Crusaders': 353,\n",
       " 'Comedy': 354,\n",
       " 'Yalta': 355,\n",
       " 'tap': 356,\n",
       " 'Hardy': 357,\n",
       " 'post': 358,\n",
       " 'inspirational': 359,\n",
       " 'Nina': 360,\n",
       " 'living': 361,\n",
       " 'books': 362,\n",
       " 'sold': 363,\n",
       " 'Taj': 364,\n",
       " 'impenetrable': 365,\n",
       " 'Gilbert': 366,\n",
       " 'information': 367,\n",
       " 'psychologically': 368,\n",
       " 'credits': 369,\n",
       " 'Yom': 370,\n",
       " 'take': 371,\n",
       " 'invaded': 372,\n",
       " 'Patti': 373,\n",
       " 'educational': 374,\n",
       " 'reliever': 375,\n",
       " 'You': 376,\n",
       " 'understand': 377,\n",
       " 'session': 378,\n",
       " 'brightest': 379,\n",
       " 'Billy': 380,\n",
       " 'stardom': 381,\n",
       " 'aged': 382,\n",
       " 'wrestler': 383,\n",
       " 'revival': 384,\n",
       " 'cullion': 385,\n",
       " 'afflict': 386,\n",
       " 'sled': 387,\n",
       " 'castle': 388,\n",
       " 'Pajamas': 389,\n",
       " 'Maltese': 390,\n",
       " 'Mackenzie': 391,\n",
       " 'Ed': 392,\n",
       " 'bless': 393,\n",
       " 'dates': 394,\n",
       " 'sugar': 395,\n",
       " 'crust': 396,\n",
       " 'anti-AIDS': 397,\n",
       " 'numbers': 398,\n",
       " 'marketed': 399,\n",
       " 'Magi': 400,\n",
       " 'Hong': 401,\n",
       " 'Brazil': 402,\n",
       " 'daycare': 403,\n",
       " 'Lucas': 404,\n",
       " 'jail': 405,\n",
       " 'wives': 406,\n",
       " 'domestication': 407,\n",
       " 'commissioner': 408,\n",
       " 'Numerals': 409,\n",
       " 'Pie': 410,\n",
       " 'first': 411,\n",
       " 'pies': 412,\n",
       " 'menace': 413,\n",
       " 'Shirley': 414,\n",
       " 'Dwarfs': 415,\n",
       " 'Jeff': 416,\n",
       " 'Younger': 417,\n",
       " 'daughters': 418,\n",
       " 'magazines': 419,\n",
       " 'Prewitt': 420,\n",
       " 'owner': 421,\n",
       " 'National': 422,\n",
       " 'church': 423,\n",
       " 'Dominica': 424,\n",
       " 'lickin': 425,\n",
       " 'issues': 426,\n",
       " 'goes': 427,\n",
       " 'Bella': 428,\n",
       " 'sagebrush': 429,\n",
       " 'oxidation': 430,\n",
       " 'sonnets': 431,\n",
       " 'Basque': 432,\n",
       " 'Gunpowder': 433,\n",
       " 'visible': 434,\n",
       " 'bowl': 435,\n",
       " 'surrounds': 436,\n",
       " 'busiest': 437,\n",
       " 'Again': 438,\n",
       " 'novels': 439,\n",
       " 'fellow': 440,\n",
       " 'rhymes': 441,\n",
       " 'we': 442,\n",
       " 'died': 443,\n",
       " 'kilamanjaro': 444,\n",
       " 'sequencing': 445,\n",
       " 'Baskin': 446,\n",
       " 'Wordsworth': 447,\n",
       " 'Label': 448,\n",
       " 'Declaration': 449,\n",
       " 'Brooklyn': 450,\n",
       " 'Superbowl': 451,\n",
       " 'march': 452,\n",
       " 'steepest': 453,\n",
       " 'Flintstones': 454,\n",
       " 'promising': 455,\n",
       " 'February': 456,\n",
       " 'Adventours': 457,\n",
       " 'Marlowe': 458,\n",
       " 'Soldiers': 459,\n",
       " 'Shooting': 460,\n",
       " 'recognize': 461,\n",
       " 'preface': 462,\n",
       " 'sing': 463,\n",
       " 'English': 464,\n",
       " 'cars': 465,\n",
       " 'hematoma': 466,\n",
       " 'March': 467,\n",
       " 'most': 468,\n",
       " 'islands': 469,\n",
       " 'similar': 470,\n",
       " 'long': 471,\n",
       " 'Montmartre': 472,\n",
       " 'Club': 473,\n",
       " 'serfdom': 474,\n",
       " 'gradual': 475,\n",
       " 'theatrical': 476,\n",
       " 'Carter': 477,\n",
       " 'drug': 478,\n",
       " 'generals': 479,\n",
       " 'golden': 480,\n",
       " 'Trek': 481,\n",
       " 'Johnny': 482,\n",
       " 'scientific': 483,\n",
       " 'black-and-white': 484,\n",
       " 'supremacy': 485,\n",
       " 'Naples': 486,\n",
       " 'Haifa': 487,\n",
       " 'southeast': 488,\n",
       " 'iris': 489,\n",
       " 'Madonna': 490,\n",
       " 'marijuana': 491,\n",
       " 'snakes': 492,\n",
       " 'nine': 493,\n",
       " 'yards': 494,\n",
       " 'fringe': 495,\n",
       " 'writer': 496,\n",
       " 'planet': 497,\n",
       " 'lovely': 498,\n",
       " 'magenta': 499,\n",
       " 'attorneys': 500,\n",
       " 'NAFTA': 501,\n",
       " 'sailed': 502,\n",
       " '43rd': 503,\n",
       " 'biggest-selling': 504,\n",
       " 'Major': 505,\n",
       " 'sins': 506,\n",
       " 'whisky': 507,\n",
       " 'whose': 508,\n",
       " 'kidnaped': 509,\n",
       " 'inhabit': 510,\n",
       " 'gives': 511,\n",
       " 'mixture': 512,\n",
       " 'frequency': 513,\n",
       " 'higher': 514,\n",
       " 'Literature': 515,\n",
       " 'fathom': 516,\n",
       " 'PSI': 517,\n",
       " 'Cowboys': 518,\n",
       " 'eczema': 519,\n",
       " 'hourly': 520,\n",
       " 'Antichrist': 521,\n",
       " 'veterans': 522,\n",
       " 'stringed': 523,\n",
       " 'tape': 524,\n",
       " 'dioxide': 525,\n",
       " 'Fifth': 526,\n",
       " 'now-defunct': 527,\n",
       " 'Matt': 528,\n",
       " 'class': 529,\n",
       " 'prosecutor': 530,\n",
       " 'Soviet': 531,\n",
       " '39': 532,\n",
       " 'communications': 533,\n",
       " 'Zimbabwe': 534,\n",
       " 'M': 535,\n",
       " 'Horlick': 536,\n",
       " 'a.m.': 537,\n",
       " 'overalls': 538,\n",
       " 'Fred': 539,\n",
       " 'LCD': 540,\n",
       " 'worm': 541,\n",
       " '17': 542,\n",
       " 'Politics': 543,\n",
       " 'firestorm': 544,\n",
       " 'shoplifts': 545,\n",
       " 'retrievers': 546,\n",
       " 'comedy': 547,\n",
       " 'Preston': 548,\n",
       " 'Metamorphosis': 549,\n",
       " 'flytrap': 550,\n",
       " '1932': 551,\n",
       " 'hero': 552,\n",
       " 'Water': 553,\n",
       " 'holidays': 554,\n",
       " 'Shakespearean': 555,\n",
       " 'commonly-spoken': 556,\n",
       " 'Organization': 557,\n",
       " 'athletic': 558,\n",
       " 'Trial': 559,\n",
       " 'employees': 560,\n",
       " 'publish': 561,\n",
       " \"'re\": 562,\n",
       " '1992': 563,\n",
       " 'taxed': 564,\n",
       " 'river': 565,\n",
       " 'translate': 566,\n",
       " 'medium': 567,\n",
       " 'Ball': 568,\n",
       " 'agent': 569,\n",
       " 'meeting': 570,\n",
       " 'tiny': 571,\n",
       " 'Sinemet': 572,\n",
       " 'wind': 573,\n",
       " 'completed': 574,\n",
       " 'insanity': 575,\n",
       " 'current': 576,\n",
       " 'ears': 577,\n",
       " 'NN': 578,\n",
       " 'Asian': 579,\n",
       " 'signals': 580,\n",
       " 'extends': 581,\n",
       " 'archenemy': 582,\n",
       " 'linked': 583,\n",
       " 'Milky': 584,\n",
       " 'Burkina': 585,\n",
       " 'houses': 586,\n",
       " 'psychology': 587,\n",
       " 'Pollux': 588,\n",
       " 'Tennessee': 589,\n",
       " 'Venice': 590,\n",
       " 'Jersey': 591,\n",
       " 'photographer': 592,\n",
       " 'Angela': 593,\n",
       " 'Research': 594,\n",
       " 'turkey': 595,\n",
       " 'Quebec': 596,\n",
       " 'fifth': 597,\n",
       " 'poetry': 598,\n",
       " 'subtitled': 599,\n",
       " 'problems': 600,\n",
       " 'erotic': 601,\n",
       " 'pulp': 602,\n",
       " 'Zionism': 603,\n",
       " 'predicted': 604,\n",
       " 'unanswerable': 605,\n",
       " 'rear': 606,\n",
       " 'Brenner': 607,\n",
       " 'tent': 608,\n",
       " 'pollution': 609,\n",
       " 'Assisi': 610,\n",
       " 'Cohan': 611,\n",
       " 'brew': 612,\n",
       " 'liner': 613,\n",
       " 'Mac': 614,\n",
       " 'bat': 615,\n",
       " 'Bogart': 616,\n",
       " 'Presley': 617,\n",
       " 'dragonflies': 618,\n",
       " 'G.M.T.': 619,\n",
       " 'Export': 620,\n",
       " 'kicked': 621,\n",
       " 'market': 622,\n",
       " 'Greenfield': 623,\n",
       " 'sprawling': 624,\n",
       " 'pleasure': 625,\n",
       " 'Lost': 626,\n",
       " 'bloom': 627,\n",
       " 'Aykroyd': 628,\n",
       " 'restaurant': 629,\n",
       " 'Around': 630,\n",
       " 'row': 631,\n",
       " 'Vera': 632,\n",
       " 'donate': 633,\n",
       " 'track': 634,\n",
       " 'walked': 635,\n",
       " 'Noble': 636,\n",
       " 'Davis': 637,\n",
       " 'keeps': 638,\n",
       " 'Nasty': 639,\n",
       " 'Levine': 640,\n",
       " 'Coulee': 641,\n",
       " 'spokesman': 642,\n",
       " 'millenium': 643,\n",
       " 'object-oriented': 644,\n",
       " 'relatives': 645,\n",
       " 'Amish': 646,\n",
       " 'Hinckley': 647,\n",
       " 'ex-dictator': 648,\n",
       " 'Flakes': 649,\n",
       " 'Bolivia': 650,\n",
       " 'fox': 651,\n",
       " 'Paul': 652,\n",
       " 'feeding': 653,\n",
       " 'double-O': 654,\n",
       " 'telephones': 655,\n",
       " 'Justice': 656,\n",
       " 'tile': 657,\n",
       " 'facility': 658,\n",
       " 'printing': 659,\n",
       " 'steps': 660,\n",
       " 'boycott': 661,\n",
       " 'states': 662,\n",
       " 'Iraqis': 663,\n",
       " '1990': 664,\n",
       " 'Randolph': 665,\n",
       " 'earth': 666,\n",
       " 'edition': 667,\n",
       " 'Family': 668,\n",
       " 'treatment': 669,\n",
       " 'Simple': 670,\n",
       " 'scholar': 671,\n",
       " 'foot': 672,\n",
       " 'rabies': 673,\n",
       " 'punch-drunk': 674,\n",
       " 'our': 675,\n",
       " 'Reims': 676,\n",
       " 'Terrified': 677,\n",
       " 'Lambs': 678,\n",
       " 'newsmen': 679,\n",
       " 'dice': 680,\n",
       " '3-pin': 681,\n",
       " 'Cartier': 682,\n",
       " 'spill': 683,\n",
       " '1939': 684,\n",
       " 'Union': 685,\n",
       " 'equity': 686,\n",
       " 'Yeat': 687,\n",
       " 'Tab': 688,\n",
       " 'Spahn': 689,\n",
       " 'Caine': 690,\n",
       " 'Dale': 691,\n",
       " 'H.G.': 692,\n",
       " 'services': 693,\n",
       " 'southwestern': 694,\n",
       " 'Monkey': 695,\n",
       " 'East': 696,\n",
       " 'autobiography': 697,\n",
       " 'Corpus': 698,\n",
       " 'social': 699,\n",
       " 'writing': 700,\n",
       " 'originally': 701,\n",
       " 'Hank': 702,\n",
       " 'freezing': 703,\n",
       " 'Hill': 704,\n",
       " 'Jellicle': 705,\n",
       " 'alphabetically': 706,\n",
       " 'Capone': 707,\n",
       " 'committee': 708,\n",
       " 'MORMONS': 709,\n",
       " 'Russell': 710,\n",
       " 'Chuck': 711,\n",
       " 'Microsoft': 712,\n",
       " 'eidologist': 713,\n",
       " 'repossession': 714,\n",
       " 'gods': 715,\n",
       " 'silversmiths': 716,\n",
       " 'steal': 717,\n",
       " 'same': 718,\n",
       " 'wheat': 719,\n",
       " 'simple': 720,\n",
       " 'occurred': 721,\n",
       " 'Tetrinet': 722,\n",
       " 'biochemists': 723,\n",
       " 'Delilah': 724,\n",
       " '8/28/1941': 725,\n",
       " '123': 726,\n",
       " 'sewer': 727,\n",
       " 'Bend': 728,\n",
       " 'Belize': 729,\n",
       " 'Theater': 730,\n",
       " '455-yard': 731,\n",
       " 'Vlaja': 732,\n",
       " 'costliest': 733,\n",
       " 'scream': 734,\n",
       " '28': 735,\n",
       " 'Strait': 736,\n",
       " 'colonists': 737,\n",
       " 'relax': 738,\n",
       " 'nerve': 739,\n",
       " 'Erykah': 740,\n",
       " 'clearer': 741,\n",
       " 'I': 742,\n",
       " 'extended': 743,\n",
       " 'cookie': 744,\n",
       " 'Foreman': 745,\n",
       " 'Lagoon': 746,\n",
       " '1956': 747,\n",
       " 'Napolean': 748,\n",
       " 'Arcadia': 749,\n",
       " 'spokespeople': 750,\n",
       " 'battery': 751,\n",
       " 'machinery': 752,\n",
       " 'ace': 753,\n",
       " 'Sunday': 754,\n",
       " 'servers': 755,\n",
       " 'some': 756,\n",
       " 'repeating': 757,\n",
       " 'manufacturer': 758,\n",
       " 'battles': 759,\n",
       " 'Brother': 760,\n",
       " 'Anthony': 761,\n",
       " 'eels': 762,\n",
       " 'properly': 763,\n",
       " '80': 764,\n",
       " 'ballet': 765,\n",
       " 'Aesop': 766,\n",
       " 'site': 767,\n",
       " 'themselves': 768,\n",
       " 'amateur': 769,\n",
       " 'doorstep': 770,\n",
       " 'Compaq': 771,\n",
       " 'recipe': 772,\n",
       " 'Guild': 773,\n",
       " 'span': 774,\n",
       " 'Dennison': 775,\n",
       " 'cookers': 776,\n",
       " 'putting': 777,\n",
       " 'Butler': 778,\n",
       " 'coprolite': 779,\n",
       " 'sired': 780,\n",
       " 'calleda': 781,\n",
       " '52': 782,\n",
       " 'papal': 783,\n",
       " 'Michener': 784,\n",
       " 'syllables': 785,\n",
       " 'brothers': 786,\n",
       " 'Will': 787,\n",
       " 'council': 788,\n",
       " 'Wendy': 789,\n",
       " 'circumcision': 790,\n",
       " 'Lactobacillus': 791,\n",
       " 'throne': 792,\n",
       " 'directly': 793,\n",
       " 'mountains': 794,\n",
       " 'bureau': 795,\n",
       " 'thee': 796,\n",
       " 'quiz': 797,\n",
       " 'Mary': 798,\n",
       " 'heaviest': 799,\n",
       " 'called': 800,\n",
       " 'III': 801,\n",
       " 'engineer': 802,\n",
       " 'perfume': 803,\n",
       " 'clockwise': 804,\n",
       " 'website': 805,\n",
       " 'organization': 806,\n",
       " 'Ursula': 807,\n",
       " 'cockatoo': 808,\n",
       " 'Lakehurst': 809,\n",
       " 'z': 810,\n",
       " 'immigration': 811,\n",
       " 'sundaes': 812,\n",
       " \"'Hara\": 813,\n",
       " 'Times': 814,\n",
       " 'times': 815,\n",
       " 'reviews': 816,\n",
       " 'Smithsonian': 817,\n",
       " 'nutrients': 818,\n",
       " 'common': 819,\n",
       " 'hendecasyllabic': 820,\n",
       " 'blow': 821,\n",
       " '1-cent': 822,\n",
       " 'military': 823,\n",
       " 'centigrade': 824,\n",
       " 'portrays': 825,\n",
       " 'Java': 826,\n",
       " 'Cisco': 827,\n",
       " 'Fordham': 828,\n",
       " 'Universe': 829,\n",
       " 'drink': 830,\n",
       " 'waterfall': 831,\n",
       " 'Scouts': 832,\n",
       " 'Dr.': 833,\n",
       " 'applied': 834,\n",
       " 'condiment': 835,\n",
       " 'tv': 836,\n",
       " 'entertainer': 837,\n",
       " 'coal': 838,\n",
       " 'chancery': 839,\n",
       " 'poop': 840,\n",
       " 'birthdate': 841,\n",
       " 'Sea': 842,\n",
       " 'Worlds': 843,\n",
       " 'treatments': 844,\n",
       " 'Dakota': 845,\n",
       " 'XV': 846,\n",
       " 'snowboard': 847,\n",
       " 'Milano': 848,\n",
       " \"'em\": 849,\n",
       " 'Barton': 850,\n",
       " 'Antonio': 851,\n",
       " 'egg': 852,\n",
       " 'chihuahuas': 853,\n",
       " '$28': 854,\n",
       " 'protagonist': 855,\n",
       " 'shortage': 856,\n",
       " 'switch': 857,\n",
       " 'Aladdin': 858,\n",
       " '72': 859,\n",
       " 'Wheel': 860,\n",
       " 'Lincoln': 861,\n",
       " 'Salzburg': 862,\n",
       " 'on-line': 863,\n",
       " 'past': 864,\n",
       " 'cognac': 865,\n",
       " 'test': 866,\n",
       " 'Lacan': 867,\n",
       " 'All': 868,\n",
       " 'Shostakovich': 869,\n",
       " 'town': 870,\n",
       " 'Rice': 871,\n",
       " 'B': 872,\n",
       " 'children': 873,\n",
       " 'W.C.': 874,\n",
       " 'yahoo.com': 875,\n",
       " 'Muslim': 876,\n",
       " 'tattoo': 877,\n",
       " 'Entebbe': 878,\n",
       " 'Mosaic': 879,\n",
       " 'Ally': 880,\n",
       " 'pacer': 881,\n",
       " 'Forests': 882,\n",
       " 'aroused': 883,\n",
       " 'reaches': 884,\n",
       " 'extant': 885,\n",
       " 'Cotrubas': 886,\n",
       " 'nun': 887,\n",
       " 'Erica': 888,\n",
       " 'cosmology': 889,\n",
       " 'conservationist': 890,\n",
       " 'what': 891,\n",
       " 'product': 892,\n",
       " 'at': 893,\n",
       " 'Sidewinder': 894,\n",
       " 'concrete': 895,\n",
       " 'Farmer': 896,\n",
       " 'marks': 897,\n",
       " 'corner': 898,\n",
       " 'Toulmin': 899,\n",
       " 'spin': 900,\n",
       " 'lasts': 901,\n",
       " 'textile': 902,\n",
       " 'Hoffman': 903,\n",
       " 'barbershop': 904,\n",
       " 'visitors': 905,\n",
       " 'epic': 906,\n",
       " 'Stonewall': 907,\n",
       " 'Wiggins': 908,\n",
       " 'Jack': 909,\n",
       " 'Command': 910,\n",
       " 'bombing': 911,\n",
       " 'condition': 912,\n",
       " 'Dutch': 913,\n",
       " 'Irkutsk': 914,\n",
       " 'NASDAQ': 915,\n",
       " 'syringe': 916,\n",
       " 'gender': 917,\n",
       " 'weigh': 918,\n",
       " 'cartoondom': 919,\n",
       " 'lawyers': 920,\n",
       " 'nickname': 921,\n",
       " 'Georgia': 922,\n",
       " 'Is': 923,\n",
       " 'wet': 924,\n",
       " 'brothers-in-law': 925,\n",
       " 'E': 926,\n",
       " 'Johnson': 927,\n",
       " 'alphabet': 928,\n",
       " 'buildings': 929,\n",
       " 'Sara': 930,\n",
       " 'shoulder': 931,\n",
       " 'mountain': 932,\n",
       " 'p.m.': 933,\n",
       " 'Enemy': 934,\n",
       " 'Poland': 935,\n",
       " 'Temple': 936,\n",
       " 'Fortune': 937,\n",
       " 'explosion': 938,\n",
       " 'forged': 939,\n",
       " 'culture': 940,\n",
       " 'indicator': 941,\n",
       " 'Freeman': 942,\n",
       " 'Marvin': 943,\n",
       " 'Smartnet': 944,\n",
       " 'partnership': 945,\n",
       " 'Third': 946,\n",
       " 'contained': 947,\n",
       " 'beautiful': 948,\n",
       " 'heart': 949,\n",
       " 'all-star': 950,\n",
       " 'ZIP': 951,\n",
       " 'phenomenon': 952,\n",
       " 'Nazis': 953,\n",
       " 'original': 954,\n",
       " 'Gina': 955,\n",
       " 'net': 956,\n",
       " 'Sheboygan': 957,\n",
       " 'nicknamed': 958,\n",
       " 'London': 959,\n",
       " 'seventeen': 960,\n",
       " 'cunnilingus': 961,\n",
       " 'cigar-chewing': 962,\n",
       " 'dead': 963,\n",
       " 'Bruce': 964,\n",
       " 'Nile': 965,\n",
       " 'how': 966,\n",
       " 'Thor': 967,\n",
       " 'Where': 968,\n",
       " 'working': 969,\n",
       " 'assassinations': 970,\n",
       " 'terata': 971,\n",
       " 'introduce': 972,\n",
       " 'practice': 973,\n",
       " 'ACLU': 974,\n",
       " \"1960's\": 975,\n",
       " 'conference': 976,\n",
       " 'tarantula': 977,\n",
       " 'initials': 978,\n",
       " 'County': 979,\n",
       " 'Douglas': 980,\n",
       " 'Lady': 981,\n",
       " 'near': 982,\n",
       " 'Archer': 983,\n",
       " 'hackers': 984,\n",
       " 'Janet': 985,\n",
       " 'beating': 986,\n",
       " 'return': 987,\n",
       " 'feature': 988,\n",
       " 'double-word-score': 989,\n",
       " 'singer': 990,\n",
       " 'Pussycats': 991,\n",
       " 'Eggs': 992,\n",
       " 'Roosevelt': 993,\n",
       " 'translated': 994,\n",
       " 'Allan': 995,\n",
       " 'Field': 996,\n",
       " 'medical': 997,\n",
       " 'Mauritania': 998,\n",
       " 'Apso': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2index2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "X+=[list(train_corp[i].tokens) for i in range(len(train_corp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "\n",
    "def pad_to_batch(batch):\n",
    "    x,y = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_x = max(4,max_x)\n",
    "\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "#             print(x[i].size(1))\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1)\n",
    "\n",
    "def pad_sample(batch):\n",
    "    max_x = max([s.size(1) for s in batch])\n",
    "#     max_x = max(5,max_x)\n",
    "\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "#             print(x[i].size(1))\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1)\n",
    "\n",
    "def prepare_sequence(seq, to_index):\n",
    "    '''\n",
    "    Converts list of tokens in to list of indicies\n",
    "    '''\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "def prepare_sequence_textacy(seq, to_index):\n",
    "    '''\n",
    "    Converts list of tokens in to list of indicies\n",
    "    '''\n",
    "    idxs = list(map(lambda w: to_index[str(w)] if to_index.get((str(w))) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "def prepare_sequence2(seq, to_index):\n",
    "    '''\n",
    "    Converts list of tokens in to list of indicies\n",
    "    '''\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DESC'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('manner How did serfdom develop in and then leave Russia ?', 'DESC')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(zip(X_2,y_2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manner', 'How', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?']\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6841, 349, 6650, 474, 7435, 1456, 9208, 1393, 7228, 6654, 4559]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = list(zip(X_2,y_2))\n",
    "print([w for w in check[0][0]])\n",
    "print(word2index2.get('n'))\n",
    "list(map(lambda w: word2index2[w] if word2index2.get(w) is not None else word2index2[\"<UNK>\"], check[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1  3964     1     1     1     1\n",
       "[torch.LongTensor of size 1x6]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence(X_2[0][0], word2index2).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index2['m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "# gpus = [0]\n",
    "# torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index[str(X[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  2\n",
       "  3\n",
       "  1\n",
       "  5\n",
       "  6\n",
       "  6\n",
       "  7\n",
       "  8\n",
       "  1\n",
       " 10\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X\n",
    "prepare_sequence_textacy(X[0], word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, y_p = [], []\n",
    "for pair in zip(X,labels):\n",
    "    X_p.append(prepare_sequence_textacy(pair[0], word2index).view(1, -1))\n",
    "#     print(X_p)\n",
    "    y_p.append(Variable(LongTensor([target2index[pair[1]]])).view(1, -1))\n",
    "    \n",
    "data_p = list(zip(X_p, y_p))\n",
    "random.shuffle(data_p)\n",
    "\n",
    "train_data = data_p[: int(len(data_p) * 0.9)]\n",
    "test_data = data_p[int(len(data_p) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  173   82    3    1   24  321\n",
       " [torch.LongTensor of size 1x6], Variable containing:\n",
       "  9\n",
       " [torch.LongTensor of size 1x1])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, y_p = [], []\n",
    "for pair in zip(X_2,y_2):\n",
    "    X_p.append(prepare_sequence(pair[0], word2index2).view(1, -1))\n",
    "    y_p.append(Variable(LongTensor([target2index2[pair[1]]])).view(1, -1))\n",
    "\n",
    "\n",
    "data_p2 = list(zip(X_p, y_p))\n",
    "# print([i[0].shape[1] for i in data_p2 if i[0].shape[1] < 4])\n",
    "# print(data_p2)\n",
    "# random.shuffle(data_p2)\n",
    "\n",
    "train_data2 = data_p2[: int(len(data_p) * 0.9)]#BE CAREFUL WITH REUSING VARIABLES\n",
    "test_data2 = data_p2[int(len(data_p) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 6490   968  8015  7878   932  7793  1456  3354  4559\n",
       "[torch.LongTensor of size 1x9]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.KeyedVectors.load_word2vec_format('/Users/andrewmendez1/Documents/CornellTech/Spring2018/CS5304/assign4/project/home/data/GoogleNews-vectors-negative300.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n",
      "Random\n"
     ]
    }
   ],
   "source": [
    "pretrained = []\n",
    "for key in index2word.keys():\n",
    "    '''\n",
    "    Here for all the words in vocab, return index, \n",
    "    then find word embedding associated with that word\n",
    "    \n",
    "    NOTE: THIS ONLY WORKS BECAUSE WHEN MAKING WORD2INDEX, \n",
    "    VALUES GUARENTEED TO BE IN ASCENDING ORDER\n",
    "    '''\n",
    "#     print(key)\n",
    "    try:\n",
    "#         print(index2word[key],model[index2word[key]].shape)\n",
    "        pretrained.append(model2[index2word[key]])\n",
    "    except:\n",
    "        print(\"Random\")\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For dummy dataset\n",
    "# pretrained = []\n",
    "# for key in index2word2.keys():\n",
    "#     '''\n",
    "#     Here for all the words in vocab, return index, \n",
    "#     then find word embedding associated with that word\n",
    "    \n",
    "#     NOTE: THIS ONLY WORKS BECAUSE WHEN MAKING WORD2INDEX, \n",
    "#     VALUES GUARENTEED TO BE IN ASCENDING ORDER\n",
    "#     '''\n",
    "# #     print(key)\n",
    "#     try:\n",
    "# #         print(index2word[key],model[index2word2[key]].shape)\n",
    "#         pretrained.append(model2[index2word[key]])\n",
    "#     except:\n",
    "#         print(\"Random\")\n",
    "#         pretrained.append(np.random.randn(300))\n",
    "        \n",
    "# pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(2, 3, 4), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 100\n",
    "BATCH_SIZE = 50\n",
    "KERNEL_SIZES = [2,3,4]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "\n",
    "model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc():\n",
    "    accuracy = 0\n",
    "    for test in test_data:\n",
    "        if test[0].shape[1]>3:# bug I need to fix\n",
    "            pred = model(test[0]).max(1)[1]\n",
    "            pred = pred.data.tolist()[0]\n",
    "            target = test[1].data.tolist()[0][0]\n",
    "            if pred == target:\n",
    "                accuracy += 1\n",
    "\n",
    "    print(accuracy/len(test_data) * 100)\n",
    "def f1():\n",
    "    predictions= []\n",
    "    y_test=[]\n",
    "    for i,test in enumerate(getBatch(1, test_data)):\n",
    "        inputs,targets = pad_to_batch(test)\n",
    "#         if test[0].shape[1]>3:# bug I need to fix\n",
    "        predictions.append(model(inputs).max(1)[1].data[0])\n",
    "#         print(targets.data.tolist()[0])\n",
    "        y_test.append(targets.data.tolist()[0])\n",
    "#         print(predictions[-1],'==',y_test[-1])\n",
    "            \n",
    "    f1 = f1_score(y_test, predictions, average=\"micro\")\n",
    "    print(\"Naive F1 score: {}\".format(f1))\n",
    "    return f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] mean_loss : 2.24\n",
      "Naive F1 score: 0.35483870967741943\n",
      "0.35483870967741943\n",
      "[10/100] mean_loss : 1.65\n",
      "Naive F1 score: 0.27419354838709675\n",
      "0.27419354838709675\n",
      "[20/100] mean_loss : 1.04\n",
      "Naive F1 score: 0.2903225806451613\n",
      "0.2903225806451613\n",
      "[30/100] mean_loss : 0.52\n",
      "Naive F1 score: 0.22580645161290322\n",
      "0.22580645161290322\n",
      "[40/100] mean_loss : 0.33\n",
      "Naive F1 score: 0.22580645161290322\n",
      "0.22580645161290322\n",
      "[50/100] mean_loss : 0.18\n",
      "Naive F1 score: 0.22580645161290322\n",
      "0.22580645161290322\n",
      "[60/100] mean_loss : 0.17\n",
      "Naive F1 score: 0.20967741935483872\n",
      "0.20967741935483872\n",
      "[70/100] mean_loss : 0.15\n",
      "Naive F1 score: 0.24193548387096775\n",
      "0.24193548387096775\n",
      "[80/100] mean_loss : 0.15\n",
      "Naive F1 score: 0.24193548387096775\n",
      "0.24193548387096775\n",
      "[90/100] mean_loss : 0.08\n",
      "Naive F1 score: 0.20967741935483872\n",
      "0.20967741935483872\n"
     ]
    }
   ],
   "source": [
    "f1s= []\n",
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "#         print(batch)\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs, True)\n",
    "        \n",
    "        loss = loss_function(preds, targets)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch%10==0:\n",
    "        print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "        losses = []\n",
    "        f1s.append(f1())\n",
    "        print(f1s[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11e3b0be0>]"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VIXZ/vHvk40EIUE0BEii7MuwQ8AdXAiitmCrIi597WpV+Lm9bdXa17fV2kVblyq19W21rdpSwI0qirhUxaUSIOwCAdkFAgKyCCHh+f2RiQ4IZAKTnMnM/bkur86cOZM8Gcqdw1nuY+6OiIgkh5SgBxARkYaj0BcRSSIKfRGRJKLQFxFJIgp9EZEkotAXEUkiCn0RkSSi0BcRSSIKfRGRJJIWzUpmNhx4EEgF/uTuvzrg9WuAMUAVsAO42t0Xhl/rDfwRyAb2AQPdffehvtfxxx/v7dq1q/tPIiKSxGbOnLnJ3XNrW89qq2Ews1RgCVAMrAFmAJfVhHp4nWx3/zT8eARwnbsPN7M0YBbwDXefY2bHAVvdvepQ36+oqMhLSkpq/wlFRORzZjbT3YtqWy+a3TuDgDJ3X+7uFcB4YGTkCjWBH3YMUPObZBgw193nhNfbfLjAFxGR+hVN6OcDqyOerwkv24+ZjTGzZcA9wPXhxV0AN7OpZjbLzH50sG9gZlebWYmZlZSXl9ftJxARkajF7ECuu49z947ALcBPwovTgNOBK8L/+zUzO+cg733U3YvcvSg3t9ZdUiIicoSiCf21QGHE84LwskMZD1wYfrwGeMvdN7n7LmAK0P9IBhURkaMXTejPADqbWXszywBGA5MjVzCzzhFPLwCWhh9PBXqZWdPwQd0hwEJERCQQtZ6y6e6VZjaW6gBPBR5z9wVmdidQ4u6TgbFmNhTYC2wBrgq/d4uZ3Uf1Lw4Hprj7i/X0s4iISC1qPWWzoemUTRGRuovlKZuNwtZdFTzw6hIWrvu09pVFRJJUVFfkNgZmxsOvl7F77z5CbbODHkdEJC4lzJZ+TlY6J3VoybSF64MeRUQkbiVM6AMUd89jWflOlpfvCHoUEZG4lFChPzSUB8C0hRsCnkREJD4lVOgXHNuUUJtsXl2k0BcROZiECn2A4lAeM1duYfOOPUGPIiISdxIy9Pc5vPbhxqBHERGJOwkX+j3aZtM2J1P79UVEDiLhQt/MGBrK4+2l5XxWoep+EZFICRf6UL2LZ/fefUwv2xT0KCIicSUhQ/+k9sfRvEkar2oXj4jIfhIy9DPSUjizWyte+3ADVfviq1BORCRICRn6UL2LZ9OOCkpXbwl6FBGRuJGwoT+kSy5pKcYr2sUjIvK5hA39nKx0Tu5wnE7dFBGJkLChD9W7eJaX72SZCthERIAED/2aAjadxSMiUi2q0Dez4Wa22MzKzOzWg7x+jZnNM7NSM5tuZqEDXj/BzHaY2Q9iNXg08ltk0aNttnbxiIiE1Rr6ZpYKjAPOA0LAZQeGOvB3d+/l7n2Be4D7Dnj9PuClGMxbZ8WhPGau2sImFbCJiES1pT8IKHP35e5eAYwHRkau4O6RN6Y9Bvj85HgzuxD4CFhw9OPW3dDuebjD64tUwCYiEk3o5wOrI56vCS/bj5mNMbNlVG/pXx9e1gy4BfjZ4b6BmV1tZiVmVlJeXh7t7FHp0Tab/BZZOnVTRIQYHsh193Hu3pHqkP9JePFPgfvd/bCnz7j7o+5e5O5Fubm5sRoJCBewdW/F9DIVsImIRBP6a4HCiOcF4WWHMh64MPz4JOAeM1sB3Aj82MzGHsGcR6U41FoFbCIiRBf6M4DOZtbezDKA0cDkyBXMrHPE0wuApQDufoa7t3P3dsADwC/c/eGYTF4HJ3VoSfPMNKYtXN/Q31pEJK6k1baCu1eGt86nAqnAY+6+wMzuBErcfTIw1syGAnuBLcBV9Tl0XaWnpnBW11a8tmgjVfuc1BQLeiQRkUDUGvoA7j4FmHLAsjsiHt8Qxdf4aV2Hi6WhoTwmz1nH7FVbKGrXMshRREQCk9BX5EY6s2su6ammC7VEJKklTehnZ6qATUQkaUIfwgVsm1TAJiLJK6lCf2j36gI2be2LSLJKqtBv2yKLnvkqYBOR5JVUoQ/VW/uzVm2hfLsK2EQk+SRd6BeHwgVsH2prX0SST9KFfqhNdQGbdvGISDJKutA3M4pDeby9dJMK2EQk6SRd6EP1Lp49lft4e2lsa5xFROJdUob+oPY1BWzaxSMiySUpQ7+mgO31D6sL2EREkkVShj5U7+LZvLOCWau2BD2KiEiDSdrQVwGbiCSjpA395uECtlcV+iKSRJI29AGGhQvYyjaqgE1EkkNSh/7QkArYRCS5JHXot8mpKWDTvXNFJDlEFfpmNtzMFptZmZndepDXrzGzeWZWambTzSwUXl5sZjPDr800s7Nj/QMcreLurZm9eqsK2EQkKdQa+maWCowDzgNCwGU1oR7h7+7ey937AvcA94WXbwK+6u69qL5Z+hMxmzxGagrYXlukXTwikvii2dIfBJS5+3J3rwDGAyMjV3D3TyOeHgN4ePlsd18XXr4AyDKzJkc/dux0b9Oc/BZZvKrQF5EkEE3o5wOrI56vCS/bj5mNMbNlVG/pX3+Qr3MRMMvd42o/SmQB266KyqDHERGpVzE7kOvu49y9I3AL8JPI18ysB/Br4PsHe6+ZXW1mJWZWUl7e8CVowz4vYNvU4N9bRKQhRRP6a4HCiOcF4WWHMh64sOaJmRUAzwL/5e7LDvYGd3/U3YvcvSg3NzeKkWJrYPuWZKuATUSSQDShPwPobGbtzSwDGA1MjlzBzDpHPL0AWBpe3gJ4EbjV3d+Jzcixl56awlndVMAmIomv1tB390pgLDAVWARMcPcFZnanmY0IrzbWzBaYWSlwM9Vn6hB+XyfgjvDpnKVm1ir2P8bRKw7l8cnOCmauVAGbiCSutGhWcvcpwJQDlt0R8fiGQ7zv58DPj2bAhjKkS3UB26uLNjCofcugxxERqRdJfUVupOaZ6ZzS8XimLdyAu3bxiEhiUuhHKA7l8dGmnSwrVwGbiCQmhX6Eod2rDze8orN4RCRBKfQjtMnJold+jk7dFJGEpdA/QHEoj9LVW9m4fXfQo4iIxJxC/wA1BWyvL9oY9CgiIjGn0D9At9bNKTg2S7t4RCQhKfQPUFPANr1MBWwikngU+gdR3L26gO2tJSpgE5HEotA/CBWwiUiiUugfRHpqCmd3a8XrH26gsmpf0OOIiMSMQv8QikOt2bJrL7NWbQ16FBGRmFHoH8KQrrlkpKYwbeH6oEcREYkZhf4hNGuSxikdj1MBm4gkFIX+YQwN5bFi8y7KNqqATUQSg0L/MIq75wEqYBORxKHQP4zWOZn0LlABm4gkDoV+LYq7q4BNRBJHVKFvZsPNbLGZlZnZrQd5/Rozmxe+B+50MwtFvHZb+H2LzezcWA7fEIp7VO/ieU0FbCKSAGoNfTNLBcYB5wEh4LLIUA/7u7v3cve+wD3AfeH3hoDRQA9gOPD78NdrNLrmNaewpQrYRCQxRLOlPwgoc/fl7l4BjAdGRq7g7p9GPD0GqDnHcSQw3t33uPtHQFn46zUaZsbQ7tUFbDv3qIBNRBq3aEI/H1gd8XxNeNl+zGyMmS2jekv/+rq8N94Vh/KoqNzH20vLgx5FROSoxOxArruPc/eOwC3AT+ryXjO72sxKzKykvDz+gnVQu5bkZKXr1E0RafSiCf21QGHE84LwskMZD1xYl/e6+6PuXuTuRbm5uVGM1LDSwgVsb3y4UQVsItKoRRP6M4DOZtbezDKoPjA7OXIFM+sc8fQCYGn48WRgtJk1MbP2QGfgg6Mfu+EVh/LYsmsvM1duCXoUEZEjllbbCu5eaWZjgalAKvCYuy8wszuBEnefDIw1s6HAXmALcFX4vQvMbAKwEKgExrh7VT39LPVqcJeaArYNnNThuKDHERE5IhZvZWJFRUVeUlIS9BgHddVjH7Bi807+/YMzMbOgxxER+ZyZzXT3otrW0xW5dVAcymPl5l0sVQGbiDRSCv06KA5VX52rC7VEpLFS6NdBXnYmfVTAJiKNmEK/jopD4QK2T1XAJiKNj0K/jopDrQF4VQVsItIIKfTrqEtes3ABm+6dKyKNj0K/jsyM4u6teWfZZhWwiUijo9A/AjUFbG8tib+eoBpvLinngVeXsG9ffF2HISLBqvWKXPmyge2OpUXTdKYt2sB5vdoEPc5+tuys4K4XF/LMrOqKo4HtWnJap+MDnkpE4oW29I9AWmoKZ3dtxetxVMDm7kyZ9zHF97/J5NJ1jDmrI9mZafxzxura3ywiSUNb+keoOJTHM7PXUrJyCycH3MWz8dPd/M/z85m6YAO98nP427dPItQ2m+27Kxk/YzXbdu0lp2l6oDOKSHzQlv4ROiOigC0o7s6EGasZet+b/HtxObee141nrzuVUNtsAEYVFVJRuY/n5xyuCVtEkolC/wg1a5LGqZ2OY9rCDQRRWrf6k118488f8KOn59KtdTYv3XAG1wzpSFrqF3+kPfNzCLXJZkKJdvGISDWF/lEoDuWx6pNdLNnQcAVsVfucx6Z/xLD732L2qi3cdWFPxl99Mh1ymx10/UsHFjJ/7acsWLetwWYUkfil0D8KQ7tXF7C9uqhhdvEs3bCdS/7wLne+sJCTOrTklZuH8I2TTyQl5dA1zyP7tiUjLYWJJWsaZEYRiW8K/aOQl51Jn8IW9X7v3L1V+3jotaVc8LvpfLRpJw9c2pfHvzmQ/BZZtb63RdMMzu3Rmmdnr2X33kZ5/xoRiSGF/lEaFspjzuqtbKinArZ5a7bx1Yem89tpSxjWI49pNw/hwn75dbqJy6VFhWz7bK/aQUVEoX+06msXz+69VfzypUWMHDedT3ZW8Og3BvDw5f05vlmTOn+tUzseR36LLB3QFZHoQt/MhpvZYjMrM7NbD/L6zWa20MzmmtlrZnZixGv3mNkCM1tkZr+zBLvPYJe8ZpzQsmlMt6LfX76Z4Q+8xR/fXM6ookKm3TyEYT1aH/HXS0kxLikqYHrZJtZs2RWzOUWk8ak19M0sFRgHnAeEgMvMLHTAarOBInfvDUwC7gm/91TgNKA30BMYCAyJ2fRxwMwoDuXxbtlmdhxlAdv23Xu5/dl5jH70farceeq7J/Gri3qTk3X0F1ZdPKAAgEkzdUBXJJlFs6U/CChz9+XuXgGMB0ZGruDub7h7zSbk+0BBzUtAJpABNAHSgYTbsVwcyqOiah9vH0UB2xsfbmTY/W/xjw9W8d3T2zP1xsEx7cwpOLYpp3c6nokla1TCJpLEogn9fCByZ/Ca8LJD+Q7wEoC7vwe8AXwc/m+quy86slHjV9GJ4QK2I9jF88nOCm4cP5tv/WUGzZqk8fS1p/KTr4RomhH7hoxLigpZu/Uz3l22OeZfW0Qah5gmi5ldCRQR3oVjZp2A7nyx5T/NzM5w97cPeN/VwNUAJ5xwQixHahBpqSmc3e2LArbIq2IPxd15Ye7H/HTyArZ9tpcbzunMdWd1pElaar3NOSyUR05WOv8sWc3pndW8KZKMotnSXwsURjwvCC/bj5kNBW4HRrj7nvDirwHvu/sOd99B9b8ATjnwve7+qLsXuXtRbm5uXX+GuFDcPY+tu/YyY8WWWtddv2033/vbTP7fP2aTf2wWL1x/OjcVd6nXwAfITE/lwr5tmbpgPVt3VdTr9xKR+BRN6M8AOptZezPLAEYDkyNXMLN+wB+pDvzIm8euAoaYWZqZpVP9L4CE270DMLhLLhlphy9gc3f+8cEqiu97k7eXlnP7+d155tpT6dY6u8HmHDUwXMJWuq7BvqeIxI9aQ9/dK4GxwFSqA3uCuy8wszvNbER4tXuBZsBEMys1s5pfCpOAZcA8YA4wx93/FesfIh4c0ySN0zoex7RF6w9awLZy804u/7//cNsz8wi1zWbqjYP53uAOUe0KiqUebXPo0VYlbCLJKqp9+u4+BZhywLI7Ih4PPcT7qoDvH82AjUlxqDVvPDuPJRt20LV1c6C6IO3xdz7iN68sJj0lhV98rRejBxYeti+nvl06sJA7nl/A/LXb6JmfE9gcItLwdEVuDA3t3gqAaQvXA7B4/Xa+/si7/PzFRZzW8XheuXkwl590QqCBDzCyT364hE1b+yLJRnfOiqFW2Zn0LWzBywvWU7nPGfdGGc0z03lwdF9G9Glbp76c+pTTNJ3hPVrzXOk6bju/O5np9XsAWUTih7b0Y6w4lMf8tZ/ywKtLOb9XG6bdNJiRfetWkNYQLh1YXcJW3w2hIhJftKUfYxf1L2Dmyi1cPugEhobygh7nkE7pcBwFx2YxYcZqRvRpG/Q4ItJAtKUfY61zMnnsmwPjOvAhXMI2oJB3lm1i9ScqYRNJFgr9JHZxkUrYRJKNQj+J5bfI4vROxzNppkrYRJKFQj/JjQqXsL2zbFPQo4hIA1DoJ7lhPfJo0TSdf87QOfsiyUChn+SapKVyYd98XlmwQSVsIklAoS+MKiqkomofz83+UnmqiCQYhb4QaptNz/xsJpToLB6RRKfQFwAuLSpk4cefMn/ttqBHEZF6pNAXAEaES9hUuSyS2BT6AlSXsJ3XszXPzV7L7r1VQY8jIvVEoS+fu7SokE93VzJ1wfqgRxGReqLQl8+d3OE4CltmaRePSAJT6MvnPi9hK9usEjaRBKXQl/1cNKAAM5ioEjaRhBRV6JvZcDNbbGZlZnbrQV6/2cwWmtlcM3vNzE6MeO0EM3vFzBaF12kXu/El1vJbZHFG51wmlaymSiVsIgmn1tA3s1RgHHAeEAIuM7PQAavNBorcvTcwCbgn4rW/Afe6e3dgELAxFoNL/RlVVMC6bbt5p0wlbCKJJpot/UFAmbsvd/cKYDwwMnIFd3/D3Wt2Ar8PFACEfzmkufu08Ho7ItaTOFUcCpew6YCuSMKJJvTzgci//WvCyw7lO8BL4cddgK1m9oyZzTaze8P/ctiPmV1tZiVmVlJeXh7t7FJPakrYpi3YwJadKmETSSQxPZBrZlcCRcC94UVpwBnAD4CBQAfgmwe+z90fdfcidy/Kzc2N5UhyhD4vYStVCZtIIokm9NcChRHPC8LL9mNmQ4HbgRHuvie8eA1QGt41VAk8B/Q/upGlIYTaZtMrP4d/zliNuw7oiiSKaEJ/BtDZzNqbWQYwGpgcuYKZ9QP+SHXgbzzgvS3MrGbz/Wxg4dGPLQ1h1MBCPly/nflrPw16FBGJkVpDP7yFPhaYCiwCJrj7AjO708xGhFe7F2gGTDSzUjObHH5vFdW7dl4zs3mAAf9XDz+H1IMRfdrSRCVsIgklLZqV3H0KMOWAZXdEPB56mPdOA3of6YASnJyscAlb6Vpuv6A7melfOgYvIo2MrsiVwxo1sJDtKmETSRgKfTmsk9tXl7DpxukiiUGhL4eVkmKMGlDIu8tUwiaSCBT6UqvPS9h0QFek0VPoS63atshicOdcJs5coxI2kUZOoS9RGVVUyMfbdjNdJWwijZpCX6IyNNSKY5umM0EHdEUaNYW+RKVJWioX9svnlYXr+UQlbCKNlkJfonbpwEL2VjnPzVYJm0hjpdCXqHVrnU3vghwmlKiETaSxUuhLnYwqqi5hm7d2W9CjiMgRUOhLnXxVJWwijZpCX+okJyud83u14fnSdezeWxX0OCJSRwp9qbNRRdUlbC/PVwmbSGOj0Jc6O6l9S05o2VQlbCKNkEJf6iwlxRhVVMB7yzezcvPOoMcRkTpQ6MsRuWhAASkGk2auCXoUEakDhb4ckTY5WQzukssklbCJNCpR3S7RzIYDDwKpwJ/c/VcHvH4z8F2gEigHvu3uKyNez6b6hujPufvYGM0uARtVVMh1T83i7aXlnNm1VdDjSCMzf+02fv3yh3FxFlinVs25c2QP0lMTfzu41tA3s1RgHFAMrAFmmNlkd18YsdpsoMjdd5nZtcA9wKURr98FvBW7sSUeDO2eR8tjMphQslqhL3WyZWcF339iJrv3VtG1dfNAZ6mscv7xwSqaZqTyP18JBTpLQ4hmS38QUObuywHMbDwwkuotdwDc/Y2I9d8Hrqx5YmYDgDzgZaAoBjNLnMhIS+HCvvk88f4KPtlZQctjMoIeSRqBffucmyaUUr59DxOvOYU+hS2CHon/fX4+f57+EQNOPJbze7UJepx6Fc2/ZfKByHPz1oSXHcp3gJcAzCwF+C3wgyMdUOJbTQnbsyphkyg9/EYZ/15czv98NRQXgQ9w+wUh+ha24EeT5rK8fEfQ49SrmO7AMrMrqd6avze86Dpgirsf9hQPM7vazErMrKS8vDyWI0k969q6OX0KcpioEjaJwttLy7n/1SVc2LctV550QtDjfC4jLYVxV/QnPdW49slZ7KqoDHqkehNN6K8FCiOeF4SX7cfMhgK3AyPcfU948SnAWDNbAfwG+C8z+9WB73X3R929yN2LcnNz6/gjSNBGDawuYZu7RiVscmjrtn7GDeNL6dyqGb/4ei/MLOiR9pPfIosHR/djycbt/OTZ+Qm7ERNN6M8AOptZezPLAEYDkyNXMLN+wB+pDvyNNcvd/Qp3P8Hd21G9i+dv7n5rzKaXuPDVPm3JTFcJmxxaReU+xvx9Fnv2VvHIlQNomhHViYMNbnCXXG44pzPPzF7L3z9YFfQ49aLW0Hf3SmAsMBVYBExw9wVmdqeZjQivdi/QDJhoZqVmNvkQX04SUHZmOuf3bMPk0nV8VhH86XcSf34xZRGzV23lnov70DG3WdDjHNb1Z3dmSJdcfjZ5IXPXbA16nJiLap++u09x9y7u3tHd7w4vu8PdJ4cfD3X3PHfvG/5vxEG+xl90jn7iuqSokO17Knl5wcdBjyJx5l9z1vGXd1fw7dPac0Hv+D8zJiXFeODSvuQ2b8K1T85i667Euj1o4l+JIA3i5A4tOfE4lbDJ/so2bufWp+cy4MRjue38bkGPE7Vjj8lg3BX92bh9Nzf9s5R9CXTVuUJfYsLMGFVUyPvLP1EJmwCwc08l1zw5i8z0VMZd3r/RXe3at7AFd3wlxBuLy/n9v8uCHidmGtefgsS1i/pXl7BNLFEJW7Jzd257Zh7Ly3fwu8v60TonM+iRjsiVJ5/IyL5t+e20JUxfuinocWJCoS8x0zonkyEqYRPgifdXMnnOOm4u7sJpnY4PepwjZmb88uu96JTbjOvHz+bjbZ8FPdJRU+hLTI0qKmT9p7t5a6kusktWs1dt4a4XFnJ2t1Zcd2anoMc5ak0z0njkygHs2VvFmKdmUVG5L+iRjopCX2LqnJoSNh3QTUqf7KxgzFOzyMvO5L5RfUhJia8LsI5Up1bN+PXFvZm1aiu/fGlR0OMcFYW+xFRGWgpf65fPq4s2sHnHntrfIAmjap9z4z9L2bSjgt9f0Z8WTROrgO8rvdvyzVPb8fg7K3hh7rqgxzliCn2JuVFFKmFLRg+9vpS3lpTzvyNC9C6IjyK1WPvx+d3pf0ILbpk0l7KNjbOYTaEvMde1dXP6FLZggkrYksabS8p58LWlfL1fPpcPip8itVirKWZrkp7KdU/NbJTFbAp9qReXFhWyZMMO5qiELeGt3foZN46fTZdWzbn7a/FXpBZrbXKyeHB0X5Zu3MGPn5nX6DZsFPpSL77Sp41K2JJAReU+xjw1i71VziNX9icrIzXokRrEGZ1zuXloF54rXceT/2lcxWwKfakX2ZnpnN+rDf9SCVtCu/vFhZSu3spvLulNhzgvUou1MWd14qyuudz1r4XMWd14itkU+lJvRoVL2F6arxK2RPR86Vr++t5Kvnt6e4b3jP8itVhLSTHuDxezXffULLbsbBzFbBZv+6OKioq8pKQk6DEkBtyds37zbz7dXUl+i6ygx4kLKQbn92rDd05vT1oj66KJtHTDdkY8/A4987P5+/dObnS9OrE0d81WLn7kPU7tdByPXTUwsGsTzGymu9d6H/L4vJOBJAQz48fnd2e8LtT63NZdFfzypQ/519x13HNRH0Jts4Meqc527KnkmidnckyTVB5uhEVqsda7oAV3fDXET56bz8NvlHH9OZ2DHumwFPpSr4b1aM2wHq2DHiNuuDsvzV/PHc/PZ8TD07n2zI6MPbsTTdIaxwFQd+fWp+fy0aadPPndk8jLbpxFarF2xUknMHPlFu5/dQn9TmjBGZ3j97avyf0rWqSBmRnn92rDtJuGMKJvWx56vYwLfjedmSu3BD1aVP767gpemPsx/z2sK6d2bLxFarFmZtz9tZ50btWMG8aXsm5r/BazKfRFAnDsMRncN6ovf/nWQD6rqOLiP7zLz/61gJ174vdin1mrtnD3lEWc060V1w7pGPQ4caemmK3mfsDxWswWVeib2XAzW2xmZWb2pRubm9nNZrbQzOaa2WtmdmJ4eV8ze8/MFoRfuzTWP4BIY3Zm11ZMvWkw3zj5RB5/ZwXnPvAWb8dhQ+nmHXsY89QsWudkct+ovglTpBZrHXObcc/FvZm9aiu/mBKfxWy1hr6ZpQLjgPOAEHCZmYUOWG02UOTuvYFJwD3h5buA/3L3HsBw4AEzS8xSDpEj1KxJGneO7MmE759CRmoK3/jzB/xw4hy27dob9GjAF0Vqm3dW8MgVA8hpmh70SHHt/F5t+PZp7fnLuyuYPCf+itmi2dIfBJS5+3J3rwDGAyMjV3D3N9x9V/jp+0BBePkSd18afrwO2AjE7xEOkQANat+SKTecwbVnduSZ2WsZev+bvDx/fdBj8eBrS3l76SZ+NqIHPfNzgh6nUbjt/G4MOPFYbn16LmUbtwc9zn6iCf18IPKcuzXhZYfyHeClAxea2SAgA1hWlwFFkklmeiq3DO/G82NOI7dZE655cibXPTWTjdt3BzLPvxdv5KHXl3JR/wJGDywMZIbGKD01hXGX9ycrPZVrnpwVV8dqYnog18yuBIqAew9Y3gZ4AviWu3/p6IaZXW1mJWZWUl4ef/szRRpaz/wcnh97Gj88tyuvLtpI8X1vMWnmmgYt91qzZRc3/rOUrnnN+fmFPRO+SC3WWudk8rvL+rG8fAe3xVExWzShvxaI/BVfEF62HzMbCtwOjHD3PRHLs4EXgdvd/f2DfQMW/rPvAAAG/0lEQVR3f9Tdi9y9KDdXe39EoHprccxZnZhy/Rl0atWMH0ycw1WPz2DNll21v/ko7amsvjVgVZXzhysHJE2RWqyd1ul4/ntYVybPWccT768MehwgutCfAXQ2s/ZmlgGMBiZHrmBm/YA/Uh34GyOWZwDPAn9z90mxG1skeXRq1YyJ3z+Fn43oQcmKTxh2/1v89d0V7KvHm8///IVFzFmzjXsv6UO744+pt++TDK4d0pFzurXirhcWMntV8Ndj1Br67l4JjAWmAouACe6+wMzuNLMR4dXuBZoBE82s1MxqfimMAgYD3wwvLzWzvrH/MUQSW0qKcdWp7Zh642AGnHgs/zt5AaP++B7LymN/96bnZq/lifdXcvXgDgzvqaupj1ZKinHfqL7kZWcy5qlZfBJwMZsK10QaGXfn6VlrueuFhXy2t4obzunM1YM7xKQDZ8mG7Yx8+B165efw9++d1KhL4eLNvDXbuOiRdzm543E8/s2BpMb4WodoC9f0JyrSyJgZFw8oYNrNgxnavRX3Tl3MyIffYf7ao7tL2RdFamk8fHk/BX6M9SrI4acjevDWknIeen1pYHPoT1WkkWrVPJPfXzGAP1zZn43b9zBy3Dv8+uUP2b237jetcXdumTSXFZt28tBl/WilIrV6cdmgQr7eP58HX1vKm0uCOVNRoS/SyA3v2YbXbh7C1/vl88i/l3H+g28zY8Undfoaj7+zghfnfcwPz+3GKR2Pq6dJxcy4+8JedM1rzo3jZ7M2gGI2hb5IAshpms69l/Thb98exJ7KfVzyh/e44/n57IjioqCZKz/hF1MWMbR7HtcM6dAA0ya3rIxUfn9Ff/ZWOWOeavhiNoW+SAIZ3CWXV24azDdPbccT76/k3PvfOuxuhE079jDmqdm0bZHFb0f10QVYDaRDbjN+c0lvSldv5e4XFzbo91boiySYY5qk8dMRPZj4/VPITE/hqsc+4OYJpV+6h2vVPueG8bP5ZFcFv7+iPzlZKlJrSMN7tuG7p7fnr++t5PnSL13vWm8U+iIJqqhdS168/gzGntWJyaXrKL7/TabM+/jzOoAHXl3CO2WbuWukitSCcst53RjY7lhufXoeSzc0TDGbQl8kgWWmp/KDc7vy/NjTaJ2TyXVPzeKaJ2cysWQ1D71exqiiAi4deELQYyat9NQUHr68P8c0SeWaJ2dGdQzmaCn0RZJAj7Y5PHfdadwyvBtvLC7nh5PmEmqTzZ0jewY9WtLLy64uZvto005ufXpuvRez6cboIkkiLTWFa8/syLk98vjruyv47hkdyExXkVo8OLXj8fxoeDd2VVThDvV5PF01DCIiCUA1DCIi8iUKfRGRJKLQFxFJIgp9EZEkotAXEUkiCn0RkSSi0BcRSSIKfRGRJBJ3F2eZWTmw8ii+xPHAphiN09jps9ifPo/96fP4QiJ8Fie6e25tK8Vd6B8tMyuJ5qq0ZKDPYn/6PPanz+MLyfRZaPeOiEgSUeiLiCSRRAz9R4MeII7os9ifPo/96fP4QtJ8Fgm3T19ERA4tEbf0RUTkEBIm9M1suJktNrMyM7s16HmCZGaFZvaGmS00swVmdkPQMwXNzFLNbLaZvRD0LEEzsxZmNsnMPjSzRWZ2StAzBcnMbgr/PZlvZv8ws8ygZ6pPCRH6ZpYKjAPOA0LAZWYWCnaqQFUC/+3uIeBkYEySfx4ANwCLgh4iTjwIvOzu3YA+JPHnYmb5wPVAkbv3BFKB0cFOVb8SIvSBQUCZuy939wpgPDAy4JkC4+4fu/us8OPtVP+lzg92quCYWQFwAfCnoGcJmpnlAIOBPwO4e4W7bw12qsClAVlmlgY0BdYFPE+9SpTQzwdWRzxfQxKHXCQzawf0A/4T7CSBegD4EbAv6EHiQHugHHg8vLvrT2Z2TNBDBcXd1wK/AVYBHwPb3P2VYKeqX4kS+nIQZtYMeBq40d0/DXqeIJjZV4CN7j4z6FniRBrQH3jE3fsBO4GkPQZmZsdSvVegPdAWOMbMrgx2qvqVKKG/FiiMeF4QXpa0zCyd6sB/yt2fCXqeAJ0GjDCzFVTv9jvbzJ4MdqRArQHWuHvNv/wmUf1LIFkNBT5y93J33ws8A5wa8Ez1KlFCfwbQ2czam1kG1QdiJgc8U2DMzKjeZ7vI3e8Lep4guftt7l7g7u2o/v/F6+6e0Ftyh+Pu64HVZtY1vOgcYGGAIwVtFXCymTUN/705hwQ/sJ0W9ACx4O6VZjYWmEr10ffH3H1BwGMF6TTgG8A8MysNL/uxu08JcCaJH/8PeCq8gbQc+FbA8wTG3f9jZpOAWVSf9TabBL86V1fkiogkkUTZvSMiIlFQ6IuIJBGFvohIElHoi4gkEYW+iEgSUeiLiCQRhb6ISBJR6IuIJJH/D+epIvVMWJjHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(f1s)),f1s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
